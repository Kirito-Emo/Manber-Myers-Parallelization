==PROF== Connected to process 467005 (/home/emo/Documents/Programmazione/progetti/HPC/Manber-Myers-Parallelization/cmake-build-debug-cuda/hpc_cuda)
==PROF== Profiling "static_kernel" - 0: 0%....50%....100% - 8 passes
==PROF== Profiling "init_ranks_kernel" - 1: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 2: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortHistogramKernel" - 3: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortExclusiveSumKe..." - 4: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 5: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 6: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 7: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 8: 0%....50%....100% - 8 passes
==PROF== Profiling "update_ranks_kernel" - 9: 0%....50%....100% - 8 passes
==PROF== Profiling "prefix_sum_kernel(int *, int)" - 10: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 11: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortHistogramKernel" - 12: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortExclusiveSumKe..." - 13: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 14: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 20: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortHistogramKernel" - 21: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortExclusiveSumKe..." - 22: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 23: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 24: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 25: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 26: 0%....50%....100% - 8 passes
==PROF== Profiling "update_ranks_kernel" - 27: 0%....50%....100% - 8 passes
==PROF== Profiling "prefix_sum_kernel(int *, int)" - 28: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 29: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortHistogramKernel" - 30: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortExclusiveSumKe..." - 31: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 32: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 33: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 34: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 35: 0%....50%....100% - 8 passes
==PROF== Profiling "update_ranks_kernel" - 36: 0%....50%....100% - 8 passes
==PROF== Profiling "prefix_sum_kernel(int *, int)" - 37: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 38: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortHistogramKernel" - 39: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortExclusiveSumKe..." - 40: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 41: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 42: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 43: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 44: 0%....50%....100% - 8 passes
==PROF== Profiling "update_ranks_kernel" - 45: 0%....50%....100% - 8 passes
==PROF== Profiling "prefix_sum_kernel(int *, int)" - 46: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 47: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortHistogramKernel" - 48: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortExclusiveSumKe..." - 49: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 50: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 51: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 52: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 53: 0%....50%....100% - 8 passes
==PROF== Profiling "update_ranks_kernel" - 54: 0%....50%....100% - 8 passes
==PROF== Profiling "prefix_sum_kernel(int *, int)" - 55: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_kernel" - 56: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_lcp_kernel" - 57: 0%....50%....100% - 8 passes
==PROF== Profiling "_kernel_agent" - 58: 0%....50%....100% - 8 passes
==PROF== Profiling "_kernel_agent" - 59: 0%....50%....100% - 8 passes
==PROF== Profiling "_kernel_agent" - 60: 0%....50%....100% - 8 passes
=== CUDA Suffix Array + LCP ===
size=100 MB   time_build=194.46 s
max_lrs_len=4   pos=8298052
Found 1 CUDA device(s):
  [0] NVIDIA GeForce RTX 3070 Laptop GPU - 7840 MB global mem, 40 SMs
==PROF== Disconnected from process 467005
[467005] hpc_cuda@127.0.0.1
  void for_each::static_kernel<for_each::policy_350_t, long, __tabulate::functor<thrust::device_ptr<int>, system::compute_sequence_value<int, void>, long>>(T2, T3) (204800, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,073,457
    Memory Throughput                 %        96.76
    DRAM Throughput                   %        96.76
    Duration                         us       967.23
    L1/TEX Cache Throughput           %        61.26
    L2 Cache Throughput               %        40.94
    SM Active Cycles              cycle 1,066,511.90
    Compute (SM) Throughput           %        22.97
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                204,800
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      52,428,800
    Uses Green Context                                             0
    Waves Per SM                                              853.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        66.19
    Achieved Active Warps Per SM           warp        31.77
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 33.81%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (66.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    6,545,202
    Total DRAM Elapsed Cycles        cycle   54,113,280
    Average L1 Active Cycles         cycle 1,066,511.90
    Total L1 Elapsed Cycles          cycle   42,791,948
    Average L2 Active Cycles         cycle   992,081.47
    Total L2 Elapsed Cycles          cycle   32,031,232
    Average SM Active Cycles         cycle 1,066,511.90
    Total SM Elapsed Cycles          cycle   42,791,948
    Average SMSP Active Cycles       cycle 1,065,612.88
    Total SMSP Elapsed Cycles        cycle  171,167,792
    -------------------------- ----------- ------------

  init_ranks_kernel(const unsigned char *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,628,504
    Memory Throughput                 %        79.75
    DRAM Throughput                   %        79.75
    Duration                         ms         1.47
    L1/TEX Cache Throughput           %        43.34
    L2 Cache Throughput               %        33.73
    SM Active Cycles              cycle 1,623,261.52
    Compute (SM) Throughput           %        30.23
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.26
    Achieved Active Warps Per SM           warp        36.60
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.74%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    8,182,924
    Total DRAM Elapsed Cycles        cycle   82,087,936
    Average L1 Active Cycles         cycle 1,623,261.52
    Total L1 Elapsed Cycles          cycle   65,040,132
    Average L2 Active Cycles         cycle 1,495,525.62
    Total L2 Elapsed Cycles          cycle   48,592,128
    Average SM Active Cycles         cycle 1,623,261.52
    Total SM Elapsed Cycles          cycle   65,040,132
    Average SMSP Active Cycles       cycle 1,622,287.36
    Total SMSP Elapsed Cycles        cycle  260,160,528
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    2,264,679
    Memory Throughput                 %        91.80
    DRAM Throughput                   %        91.80
    Duration                         ms         2.04
    L1/TEX Cache Throughput           %        34.36
    L2 Cache Throughput               %        39.38
    SM Active Cycles              cycle 2,259,703.35
    Compute (SM) Throughput           %        28.97
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.05
    Achieved Active Warps Per SM           warp        37.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   13,099,222
    Total DRAM Elapsed Cycles        cycle  114,160,640
    Average L1 Active Cycles         cycle 2,259,703.35
    Total L1 Elapsed Cycles          cycle   90,493,598
    Average L2 Active Cycles         cycle 2,107,310.91
    Total L2 Elapsed Cycles          cycle   67,575,392
    Average SM Active Cycles         cycle 2,259,703.35
    Total SM Elapsed Cycles          cycle   90,493,598
    Average SMSP Active Cycles       cycle 2,259,097.34
    Total SMSP Elapsed Cycles        cycle  361,974,392
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortHistogramKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, unsigned long long, detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5) (400, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,172,775
    Memory Throughput                 %        89.00
    DRAM Throughput                   %        89.00
    Duration                         ms         1.06
    L1/TEX Cache Throughput           %        76.53
    L2 Cache Throughput               %        41.90
    SM Active Cycles              cycle 1,072,819.93
    Compute (SM) Throughput           %        70.01
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    400
    Registers Per Thread             register/thread              47
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            4.10
    # SMs                                         SM              40
    Threads                                   thread          51,200
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        73.35
    Achieved Active Warps Per SM           warp        35.21
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    6,577,412
    Total DRAM Elapsed Cycles        cycle   59,124,736
    Average L1 Active Cycles         cycle 1,072,819.93
    Total L1 Elapsed Cycles          cycle   46,906,272
    Average L2 Active Cycles         cycle 1,077,717.50
    Total L2 Elapsed Cycles          cycle   34,994,784
    Average SM Active Cycles         cycle 1,072,819.93
    Total SM Elapsed Cycles          cycle   46,906,272
    Average SMSP Active Cycles       cycle 1,073,508.85
    Total SMSP Elapsed Cycles        cycle  187,625,088
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.664%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 8.38% above the average, while the minimum instance value is 6.17% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.455%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 8.14% above the average, while the minimum instance value is 5.88% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.664%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.38% above the average, while the minimum instance value is 6.17% below the        
          average.                                                                                                      

  void cub::DeviceRadixSortExclusiveSumKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, unsigned long long>(T2 *) (4, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.83
    SM Frequency                    Ghz         1.08
    Elapsed Cycles                cycle        3,678
    Memory Throughput                 %         1.86
    DRAM Throughput                   %         0.94
    Duration                         us         3.39
    L1/TEX Cache Throughput           %         9.02
    L2 Cache Throughput               %         1.86
    SM Active Cycles              cycle       184.03
    Compute (SM) Throughput           %         0.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      4
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.34
    # SMs                                         SM              40
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 90%                                                                                             
          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.48
    Achieved Active Warps Per SM           warp         7.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          218
    Total DRAM Elapsed Cycles        cycle      185,344
    Average L1 Active Cycles         cycle       184.03
    Total L1 Elapsed Cycles          cycle      143,510
    Average L2 Active Cycles         cycle       323.56
    Total L2 Elapsed Cycles          cycle      109,760
    Average SM Active Cycles         cycle       184.03
    Total SM Elapsed Cycles          cycle      143,510
    Average SMSP Active Cycles       cycle       188.55
    Total SMSP Elapsed Cycles        cycle      574,040
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.035%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 53.38% above the average, while the minimum instance value is 88.56% below the      
          average.                                                                                                      

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,892,236
    Memory Throughput                 %        86.65
    DRAM Throughput                   %        86.65
    Duration                         ms         4.41
    L1/TEX Cache Throughput           %        63.38
    L2 Cache Throughput               %        45.08
    SM Active Cycles              cycle 4,905,048.70
    Compute (SM) Throughput           %        63.44
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.37
    Achieved Active Warps Per SM           warp        23.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,709,246
    Total DRAM Elapsed Cycles        cycle  246,603,776
    Average L1 Active Cycles         cycle 4,905,048.70
    Total L1 Elapsed Cycles          cycle  195,919,072
    Average L2 Active Cycles         cycle 4,562,698.50
    Total L2 Elapsed Cycles          cycle  145,974,752
    Average SM Active Cycles         cycle 4,905,048.70
    Total SM Elapsed Cycles          cycle  195,919,072
    Average SMSP Active Cycles       cycle 4,911,282.39
    Total SMSP Elapsed Cycles        cycle  783,676,288
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,538,596
    Memory Throughput                 %        92.90
    DRAM Throughput                   %        92.90
    Duration                         ms         4.09
    L1/TEX Cache Throughput           %        37.40
    L2 Cache Throughput               %        42.25
    SM Active Cycles              cycle 4,527,300.92
    Compute (SM) Throughput           %        31.38
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.09
    Achieved Active Warps Per SM           warp        21.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,566,246
    Total DRAM Elapsed Cycles        cycle  228,777,984
    Average L1 Active Cycles         cycle 4,527,300.92
    Total L1 Elapsed Cycles          cycle  181,359,214
    Average L2 Active Cycles         cycle 4,224,663.31
    Total L2 Elapsed Cycles          cycle  135,422,656
    Average SM Active Cycles         cycle 4,527,300.92
    Total SM Elapsed Cycles          cycle  181,359,214
    Average SMSP Active Cycles       cycle 4,528,453.20
    Total SMSP Elapsed Cycles        cycle  725,436,856
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,899,622
    Memory Throughput                 %        86.51
    DRAM Throughput                   %        86.51
    Duration                         ms         4.41
    L1/TEX Cache Throughput           %        63.36
    L2 Cache Throughput               %        45.03
    SM Active Cycles              cycle 4,903,925.80
    Compute (SM) Throughput           %        63.48
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.37
    Achieved Active Warps Per SM           warp        23.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,706,240
    Total DRAM Elapsed Cycles        cycle  246,976,512
    Average L1 Active Cycles         cycle 4,903,925.80
    Total L1 Elapsed Cycles          cycle  195,899,072
    Average L2 Active Cycles         cycle 4,563,527.72
    Total L2 Elapsed Cycles          cycle  146,195,648
    Average SM Active Cycles         cycle 4,903,925.80
    Total SM Elapsed Cycles          cycle  195,899,072
    Average SMSP Active Cycles       cycle 4,902,783.90
    Total SMSP Elapsed Cycles        cycle  783,596,288
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,534,830
    Memory Throughput                 %        92.97
    DRAM Throughput                   %        92.97
    Duration                         ms         4.09
    L1/TEX Cache Throughput           %        37.35
    L2 Cache Throughput               %        42.26
    SM Active Cycles              cycle 4,522,621.85
    Compute (SM) Throughput           %        31.44
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.23
    Achieved Active Warps Per SM           warp        21.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,565,140
    Total DRAM Elapsed Cycles        cycle  228,587,520
    Average L1 Active Cycles         cycle 4,522,621.85
    Total L1 Elapsed Cycles          cycle  181,570,146
    Average L2 Active Cycles         cycle 4,218,255.19
    Total L2 Elapsed Cycles          cycle  135,310,528
    Average SM Active Cycles         cycle 4,522,621.85
    Total SM Elapsed Cycles          cycle  181,570,146
    Average SMSP Active Cycles       cycle 4,524,294.11
    Total SMSP Elapsed Cycles        cycle  726,280,584
    -------------------------- ----------- ------------

  update_ranks_kernel(const int *, const int *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle    67,578,715
    Memory Throughput                 %         43.50
    DRAM Throughput                   %         28.00
    Duration                         ms         60.88
    L1/TEX Cache Throughput           %         25.32
    L2 Cache Throughput               %         43.50
    SM Active Cycles              cycle 67,568,209.03
    Compute (SM) Throughput           %          1.94
    ----------------------- ----------- -------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.67
    Achieved Active Warps Per SM           warp        38.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle    119,231,282
    Total DRAM Elapsed Cycles        cycle  3,406,457,856
    Average L1 Active Cycles         cycle  67,568,209.03
    Total L1 Elapsed Cycles          cycle  2,701,987,652
    Average L2 Active Cycles         cycle  63,015,850.66
    Total L2 Elapsed Cycles          cycle  2,016,404,224
    Average SM Active Cycles         cycle  67,568,209.03
    Total SM Elapsed Cycles          cycle  2,701,987,652
    Average SMSP Active Cycles       cycle  67,535,799.14
    Total SMSP Elapsed Cycles        cycle 10,807,950,608
    -------------------------- ----------- --------------

  prefix_sum_kernel(int *, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle 3,994,481,491
    Memory Throughput                 %          0.26
    DRAM Throughput                   %          0.06
    Duration                          s          3.60
    L1/TEX Cache Throughput           %         10.50
    L2 Cache Throughput               %          0.08
    SM Active Cycles              cycle 99,860,610.92
    Compute (SM) Throughput           %          0.26
    ----------------------- ----------- -------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread               1
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 96.88%                                                                                          
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp            1
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.75%                                                                                    
          The difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (33.3%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ---------------
    Metric Name                Metric Unit    Metric Value
    -------------------------- ----------- ---------------
    Average DRAM Active Cycles       cycle      14,096,682
    Total DRAM Elapsed Cycles        cycle 201,350,713,344
    Average L1 Active Cycles         cycle   99,860,610.92
    Total L1 Elapsed Cycles          cycle 159,775,231,932
    Average L2 Active Cycles         cycle   74,787,974.81
    Total L2 Elapsed Cycles          cycle 119,186,690,976
    Average SM Active Cycles         cycle   99,860,610.92
    Total SM Elapsed Cycles          cycle 159,775,231,932
    Average SMSP Active Cycles       cycle   24,964,815.23
    Total SMSP Elapsed Cycles        cycle 639,100,927,728
    -------------------------- ----------- ---------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    2,262,181
    Memory Throughput                 %        91.89
    DRAM Throughput                   %        91.89
    Duration                         ms         2.04
    L1/TEX Cache Throughput           %        34.37
    L2 Cache Throughput               %        38.31
    SM Active Cycles              cycle 2,257,663.80
    Compute (SM) Throughput           %        28.98
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.20
    Achieved Active Warps Per SM           warp        37.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   13,098,452
    Total DRAM Elapsed Cycles        cycle  114,030,592
    Average L1 Active Cycles         cycle 2,257,663.80
    Total L1 Elapsed Cycles          cycle   90,471,480
    Average L2 Active Cycles         cycle 2,166,115.19
    Total L2 Elapsed Cycles          cycle   69,454,848
    Average SM Active Cycles         cycle 2,257,663.80
    Total SM Elapsed Cycles          cycle   90,471,480
    Average SMSP Active Cycles       cycle 2,257,474.61
    Total SMSP Elapsed Cycles        cycle  361,885,920
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortHistogramKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, unsigned long long, detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5) (400, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,156,912
    Memory Throughput                 %        90.22
    DRAM Throughput                   %        90.22
    Duration                         ms         1.04
    L1/TEX Cache Throughput           %        76.93
    L2 Cache Throughput               %        43.25
    SM Active Cycles              cycle 1,066,925.93
    Compute (SM) Throughput           %        71.06
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    400
    Registers Per Thread             register/thread              47
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            4.10
    # SMs                                         SM              40
    Threads                                   thread          51,200
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        74.57
    Achieved Active Warps Per SM           warp        35.79
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    6,577,530
    Total DRAM Elapsed Cycles        cycle   58,321,920
    Average L1 Active Cycles         cycle 1,066,925.93
    Total L1 Elapsed Cycles          cycle   46,205,684
    Average L2 Active Cycles         cycle 1,095,785.53
    Total L2 Elapsed Cycles          cycle   35,521,056
    Average SM Active Cycles         cycle 1,066,925.93
    Total SM Elapsed Cycles          cycle   46,205,684
    Average SMSP Active Cycles       cycle 1,066,719.92
    Total SMSP Elapsed Cycles        cycle  184,822,736
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.684%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 7.24% above the average, while the minimum instance value is 5.37% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.788%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 7.35% above the average, while the minimum instance value is 5.28% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.684%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 7.24% above the average, while the minimum instance value is 5.37% below    
          the average.                                                                                                  

  void cub::DeviceRadixSortExclusiveSumKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, unsigned long long>(T2 *) (4, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.87
    SM Frequency                    Ghz         1.09
    Elapsed Cycles                cycle        3,699
    Memory Throughput                 %         1.74
    DRAM Throughput                   %         0.94
    Duration                         us         3.39
    L1/TEX Cache Throughput           %         8.81
    L2 Cache Throughput               %         1.74
    SM Active Cycles              cycle       188.38
    Compute (SM) Throughput           %         0.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      4
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.34
    # SMs                                         SM              40
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 90%                                                                                             
          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.48
    Achieved Active Warps Per SM           warp         7.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          220
    Total DRAM Elapsed Cycles        cycle      186,368
    Average L1 Active Cycles         cycle       188.38
    Total L1 Elapsed Cycles          cycle      143,496
    Average L2 Active Cycles         cycle       404.19
    Total L2 Elapsed Cycles          cycle      113,600
    Average SM Active Cycles         cycle       188.38
    Total SM Elapsed Cycles          cycle      143,496
    Average SMSP Active Cycles       cycle       185.18
    Total SMSP Elapsed Cycles        cycle      573,984
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.306%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.17% above the average, while the minimum instance value is 81.69% below the      
          average.                                                                                                      

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,547,337
    Memory Throughput                 %        92.71
    DRAM Throughput                   %        92.71
    Duration                         ms         4.10
    L1/TEX Cache Throughput           %        41.93
    L2 Cache Throughput               %        42.23
    SM Active Cycles              cycle 4,534,449.85
    Compute (SM) Throughput           %        31.57
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.75
    Achieved Active Warps Per SM           warp        21.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,563,140
    Total DRAM Elapsed Cycles        cycle  229,217,280
    Average L1 Active Cycles         cycle 4,534,449.85
    Total L1 Elapsed Cycles          cycle  181,490,288
    Average L2 Active Cycles         cycle 4,345,795.81
    Total L2 Elapsed Cycles          cycle  139,616,544
    Average SM Active Cycles         cycle 4,534,449.85
    Total SM Elapsed Cycles          cycle  181,490,288
    Average SMSP Active Cycles       cycle 4,537,028.92
    Total SMSP Elapsed Cycles        cycle  725,961,152
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,545,017
    Memory Throughput                 %        92.77
    DRAM Throughput                   %        92.77
    Duration                         ms         4.09
    L1/TEX Cache Throughput           %        37.49
    L2 Cache Throughput               %        41.09
    SM Active Cycles              cycle 4,544,194.85
    Compute (SM) Throughput           %        31.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.29
    Achieved Active Warps Per SM           warp        21.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,567,078
    Total DRAM Elapsed Cycles        cycle  229,101,568
    Average L1 Active Cycles         cycle 4,544,194.85
    Total L1 Elapsed Cycles          cycle  181,745,154
    Average L2 Active Cycles         cycle 4,344,319.34
    Total L2 Elapsed Cycles          cycle  139,544,832
    Average SM Active Cycles         cycle 4,544,194.85
    Total SM Elapsed Cycles          cycle  181,745,154
    Average SMSP Active Cycles       cycle 4,535,806.11
    Total SMSP Elapsed Cycles        cycle  726,980,616
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,537,072
    Memory Throughput                 %        92.90
    DRAM Throughput                   %        92.90
    Duration                         ms         4.09
    L1/TEX Cache Throughput           %        41.92
    L2 Cache Throughput               %        42.35
    SM Active Cycles              cycle 4,540,921.95
    Compute (SM) Throughput           %        31.31
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.64
    Achieved Active Warps Per SM           warp        21.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,558,336
    Total DRAM Elapsed Cycles        cycle  228,700,160
    Average L1 Active Cycles         cycle 4,540,921.95
    Total L1 Elapsed Cycles          cycle  181,595,554
    Average L2 Active Cycles         cycle 4,341,580.47
    Total L2 Elapsed Cycles          cycle  139,301,120
    Average SM Active Cycles         cycle 4,540,921.95
    Total SM Elapsed Cycles          cycle  181,595,554
    Average SMSP Active Cycles       cycle 4,524,585.28
    Total SMSP Elapsed Cycles        cycle  726,382,216
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,551,047
    Memory Throughput                 %        92.61
    DRAM Throughput                   %        92.61
    Duration                         ms         4.10
    L1/TEX Cache Throughput           %        42.01
    L2 Cache Throughput               %        42.33
    SM Active Cycles              cycle 4,532,895.38
    Compute (SM) Throughput           %        31.52
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.66
    Achieved Active Warps Per SM           warp        21.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,555,536
    Total DRAM Elapsed Cycles        cycle  229,404,672
    Average L1 Active Cycles         cycle 4,532,895.38
    Total L1 Elapsed Cycles          cycle  181,823,174
    Average L2 Active Cycles         cycle 4,351,541.34
    Total L2 Elapsed Cycles          cycle  139,730,368
    Average SM Active Cycles         cycle 4,532,895.38
    Total SM Elapsed Cycles          cycle  181,823,174
    Average SMSP Active Cycles       cycle 4,531,398.02
    Total SMSP Elapsed Cycles        cycle  727,292,696
    -------------------------- ----------- ------------

  update_ranks_kernel(const int *, const int *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle    66,725,919
    Memory Throughput                 %         43.48
    DRAM Throughput                   %         28.36
    Duration                         ms         60.11
    L1/TEX Cache Throughput           %         25.64
    L2 Cache Throughput               %         43.48
    SM Active Cycles              cycle 66,718,502.38
    Compute (SM) Throughput           %          1.96
    ----------------------- ----------- -------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.70
    Achieved Active Warps Per SM           warp        38.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle    119,241,176
    Total DRAM Elapsed Cycles        cycle  3,363,469,312
    Average L1 Active Cycles         cycle  66,718,502.38
    Total L1 Elapsed Cycles          cycle  2,668,328,238
    Average L2 Active Cycles         cycle  63,999,248.47
    Total L2 Elapsed Cycles          cycle  2,048,665,888
    Average SM Active Cycles         cycle  66,718,502.38
    Total SM Elapsed Cycles          cycle  2,668,328,238
    Average SMSP Active Cycles       cycle  66,690,831.16
    Total SMSP Elapsed Cycles        cycle 10,673,312,952
    -------------------------- ----------- --------------

  prefix_sum_kernel(int *, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle 3,972,832,664
    Memory Throughput                 %          0.26
    DRAM Throughput                   %          0.06
    Duration                          s          3.58
    L1/TEX Cache Throughput           %         10.56
    L2 Cache Throughput               %          0.08
    SM Active Cycles              cycle 99,318,437.15
    Compute (SM) Throughput           %          0.26
    ----------------------- ----------- -------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread               1
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 96.88%                                                                                          
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp            1
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.75%                                                                                    
          The difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (33.3%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ---------------
    Metric Name                Metric Unit    Metric Value
    -------------------------- ----------- ---------------
    Average DRAM Active Cycles       cycle      14,090,072
    Total DRAM Elapsed Cycles        cycle 200,259,453,952
    Average L1 Active Cycles         cycle   99,318,437.15
    Total L1 Elapsed Cycles          cycle 158,908,062,626
    Average L2 Active Cycles         cycle      76,343,825
    Total L2 Elapsed Cycles          cycle 121,976,700,288
    Average SM Active Cycles         cycle   99,318,437.15
    Total SM Elapsed Cycles          cycle 158,908,062,626
    Average SMSP Active Cycles       cycle   24,829,557.45
    Total SMSP Elapsed Cycles        cycle 635,632,250,504
    -------------------------- ----------- ---------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    2,263,425
    Memory Throughput                 %        91.84
    DRAM Throughput                   %        91.84
    Duration                         ms         2.04
    L1/TEX Cache Throughput           %        34.41
    L2 Cache Throughput               %        38.29
    SM Active Cycles              cycle 2,257,944.83
    Compute (SM) Throughput           %        29.00
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.23
    Achieved Active Warps Per SM           warp        37.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   13,098,292
    Total DRAM Elapsed Cycles        cycle  114,094,080
    Average L1 Active Cycles         cycle 2,257,944.83
    Total L1 Elapsed Cycles          cycle   90,390,180
    Average L2 Active Cycles         cycle 2,166,480.81
    Total L2 Elapsed Cycles          cycle   69,493,056
    Average SM Active Cycles         cycle 2,257,944.83
    Total SM Elapsed Cycles          cycle   90,390,180
    Average SMSP Active Cycles       cycle 2,257,425.99
    Total SMSP Elapsed Cycles        cycle  361,560,720
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortHistogramKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, unsigned long long, detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5) (400, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,163,470
    Memory Throughput                 %        89.79
    DRAM Throughput                   %        89.79
    Duration                         ms         1.05
    L1/TEX Cache Throughput           %        76.93
    L2 Cache Throughput               %        43.33
    SM Active Cycles              cycle 1,067,241.57
    Compute (SM) Throughput           %        71.20
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    400
    Registers Per Thread             register/thread              47
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            4.10
    # SMs                                         SM              40
    Threads                                   thread          51,200
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        74.78
    Achieved Active Warps Per SM           warp        35.89
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    6,582,778
    Total DRAM Elapsed Cycles        cycle   58,653,696
    Average L1 Active Cycles         cycle 1,067,241.57
    Total L1 Elapsed Cycles          cycle   46,125,850
    Average L2 Active Cycles         cycle 1,098,578.81
    Total L2 Elapsed Cycles          cycle   35,721,824
    Average SM Active Cycles         cycle 1,067,241.57
    Total SM Elapsed Cycles          cycle   46,125,850
    Average SMSP Active Cycles       cycle 1,067,584.73
    Total SMSP Elapsed Cycles        cycle  184,503,400
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.884%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 7.44% above the average, while the minimum instance value is 5.21% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.707%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 7.24% above the average, while the minimum instance value is 5.09% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.884%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 7.44% above the average, while the minimum instance value is 5.21% below the        
          average.                                                                                                      

  void cub::DeviceRadixSortExclusiveSumKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, unsigned long long>(T2 *) (4, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.81
    SM Frequency                    Ghz         1.09
    Elapsed Cycles                cycle        3,616
    Memory Throughput                 %         1.78
    DRAM Throughput                   %         0.96
    Duration                         us         3.33
    L1/TEX Cache Throughput           %         9.08
    L2 Cache Throughput               %         1.78
    SM Active Cycles              cycle       182.75
    Compute (SM) Throughput           %         0.40
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      4
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.34
    # SMs                                         SM              40
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 90%                                                                                             
          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.47
    Achieved Active Warps Per SM           warp         7.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          218
    Total DRAM Elapsed Cycles        cycle      181,248
    Average L1 Active Cycles         cycle       182.75
    Total L1 Elapsed Cycles          cycle      147,772
    Average L2 Active Cycles         cycle       338.78
    Total L2 Elapsed Cycles          cycle      111,008
    Average SM Active Cycles         cycle       182.75
    Total SM Elapsed Cycles          cycle      147,772
    Average SMSP Active Cycles       cycle       180.57
    Total SMSP Elapsed Cycles        cycle      591,088
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.026%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 51.46% above the average, while the minimum instance value is 89.08% below the      
          average.                                                                                                      

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,546,708
    Memory Throughput                 %        92.71
    DRAM Throughput                   %        92.71
    Duration                         ms         4.10
    L1/TEX Cache Throughput           %        42.16
    L2 Cache Throughput               %        42.29
    SM Active Cycles              cycle 4,543,748.20
    Compute (SM) Throughput           %        31.67
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.47
    Achieved Active Warps Per SM           warp        21.83
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,560,526
    Total DRAM Elapsed Cycles        cycle  229,186,560
    Average L1 Active Cycles         cycle 4,543,748.20
    Total L1 Elapsed Cycles          cycle  181,751,880
    Average L2 Active Cycles         cycle 4,349,241.47
    Total L2 Elapsed Cycles          cycle  139,596,544
    Average SM Active Cycles         cycle 4,543,748.20
    Total SM Elapsed Cycles          cycle  181,751,880
    Average SMSP Active Cycles       cycle 4,537,365.58
    Total SMSP Elapsed Cycles        cycle  727,007,520
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,539,989
    Memory Throughput                 %        92.89
    DRAM Throughput                   %        92.89
    Duration                         ms         4.09
    L1/TEX Cache Throughput           %        42.24
    L2 Cache Throughput               %        42.33
    SM Active Cycles              cycle 4,528,969.38
    Compute (SM) Throughput           %        31.65
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.63
    Achieved Active Warps Per SM           warp        21.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,572,712
    Total DRAM Elapsed Cycles        cycle  228,844,544
    Average L1 Active Cycles         cycle 4,528,969.38
    Total L1 Elapsed Cycles          cycle  181,407,338
    Average L2 Active Cycles         cycle 4,344,706.62
    Total L2 Elapsed Cycles          cycle  139,389,600
    Average SM Active Cycles         cycle 4,528,969.38
    Total SM Elapsed Cycles          cycle  181,407,338
    Average SMSP Active Cycles       cycle 4,529,395.07
    Total SMSP Elapsed Cycles        cycle  725,629,352
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,544,323
    Memory Throughput                 %        92.76
    DRAM Throughput                   %        92.76
    Duration                         ms         4.09
    L1/TEX Cache Throughput           %        37.60
    L2 Cache Throughput               %        41.42
    SM Active Cycles              cycle 4,531,561.80
    Compute (SM) Throughput           %        31.47
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.46
    Achieved Active Warps Per SM           warp        21.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,560,112
    Total DRAM Elapsed Cycles        cycle  229,065,728
    Average L1 Active Cycles         cycle 4,531,561.80
    Total L1 Elapsed Cycles          cycle  182,158,032
    Average L2 Active Cycles         cycle 4,351,078.28
    Total L2 Elapsed Cycles          cycle  139,523,328
    Average SM Active Cycles         cycle 4,531,561.80
    Total SM Elapsed Cycles          cycle  182,158,032
    Average SMSP Active Cycles       cycle 4,536,927.41
    Total SMSP Elapsed Cycles        cycle  728,632,128
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,536,960
    Memory Throughput                 %        92.87
    DRAM Throughput                   %        92.87
    Duration                         ms         4.09
    L1/TEX Cache Throughput           %        42.17
    L2 Cache Throughput               %        42.38
    SM Active Cycles              cycle 4,530,263.85
    Compute (SM) Throughput           %        31.61
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.88
    Achieved Active Warps Per SM           warp        22.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,554,846
    Total DRAM Elapsed Cycles        cycle  228,746,240
    Average L1 Active Cycles         cycle 4,530,263.85
    Total L1 Elapsed Cycles          cycle  181,535,910
    Average L2 Active Cycles         cycle 4,345,395.03
    Total L2 Elapsed Cycles          cycle  139,322,144
    Average SM Active Cycles         cycle 4,530,263.85
    Total SM Elapsed Cycles          cycle  181,535,910
    Average SMSP Active Cycles       cycle 4,539,807.75
    Total SMSP Elapsed Cycles        cycle  726,143,640
    -------------------------- ----------- ------------

  update_ranks_kernel(const int *, const int *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle    66,725,103
    Memory Throughput                 %         43.44
    DRAM Throughput                   %         28.36
    Duration                         ms         60.11
    L1/TEX Cache Throughput           %         25.64
    L2 Cache Throughput               %         43.44
    SM Active Cycles              cycle 66,723,912.62
    Compute (SM) Throughput           %          1.97
    ----------------------- ----------- -------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.67
    Achieved Active Warps Per SM           warp        38.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle    119,238,608
    Total DRAM Elapsed Cycles        cycle  3,363,427,328
    Average L1 Active Cycles         cycle  66,723,912.62
    Total L1 Elapsed Cycles          cycle  2,667,881,018
    Average L2 Active Cycles         cycle  63,982,987.09
    Total L2 Elapsed Cycles          cycle  2,048,640,992
    Average SM Active Cycles         cycle  66,723,912.62
    Total SM Elapsed Cycles          cycle  2,667,881,018
    Average SMSP Active Cycles       cycle  66,702,248.72
    Total SMSP Elapsed Cycles        cycle 10,671,524,072
    -------------------------- ----------- --------------

  prefix_sum_kernel(int *, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle 3,972,680,854
    Memory Throughput                 %          0.26
    DRAM Throughput                   %          0.06
    Duration                          s          3.58
    L1/TEX Cache Throughput           %         10.56
    L2 Cache Throughput               %          0.08
    SM Active Cycles              cycle 99,318,908.55
    Compute (SM) Throughput           %          0.26
    ----------------------- ----------- -------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread               1
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 96.88%                                                                                          
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp            1
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.75%                                                                                    
          The difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (33.3%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ---------------
    Metric Name                Metric Unit    Metric Value
    -------------------------- ----------- ---------------
    Average DRAM Active Cycles       cycle      14,073,304
    Total DRAM Elapsed Cycles        cycle 200,251,802,624
    Average L1 Active Cycles         cycle   99,318,908.55
    Total L1 Elapsed Cycles          cycle 158,913,384,152
    Average L2 Active Cycles         cycle   76,357,618.09
    Total L2 Elapsed Cycles          cycle 121,972,039,040
    Average SM Active Cycles         cycle   99,318,908.55
    Total SM Elapsed Cycles          cycle 158,913,384,152
    Average SMSP Active Cycles       cycle   24,829,051.99
    Total SMSP Elapsed Cycles        cycle 635,653,536,608
    -------------------------- ----------- ---------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    2,262,826
    Memory Throughput                 %        91.87
    DRAM Throughput                   %        91.87
    Duration                         ms         2.04
    L1/TEX Cache Throughput           %        34.40
    L2 Cache Throughput               %        38.30
    SM Active Cycles              cycle 2,257,610.23
    Compute (SM) Throughput           %        29.00
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.23
    Achieved Active Warps Per SM           warp        37.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   13,098,440
    Total DRAM Elapsed Cycles        cycle  114,064,384
    Average L1 Active Cycles         cycle 2,257,610.23
    Total L1 Elapsed Cycles          cycle   90,396,884
    Average L2 Active Cycles         cycle 2,165,932.34
    Total L2 Elapsed Cycles          cycle   69,474,720
    Average SM Active Cycles         cycle 2,257,610.23
    Total SM Elapsed Cycles          cycle   90,396,884
    Average SMSP Active Cycles       cycle 2,257,440.04
    Total SMSP Elapsed Cycles        cycle  361,587,536
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortHistogramKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, unsigned long long, detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5) (400, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,156,138
    Memory Throughput                 %        90.30
    DRAM Throughput                   %        90.30
    Duration                         ms         1.04
    L1/TEX Cache Throughput           %        76.95
    L2 Cache Throughput               %        43.34
    SM Active Cycles              cycle 1,067,016.50
    Compute (SM) Throughput           %        71.07
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    400
    Registers Per Thread             register/thread              47
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            4.10
    # SMs                                         SM              40
    Threads                                   thread          51,200
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        74.36
    Achieved Active Warps Per SM           warp        35.69
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    6,578,596
    Total DRAM Elapsed Cycles        cycle   58,283,008
    Average L1 Active Cycles         cycle 1,067,016.50
    Total L1 Elapsed Cycles          cycle   46,213,388
    Average L2 Active Cycles         cycle 1,098,889.50
    Total L2 Elapsed Cycles          cycle   35,496,992
    Average SM Active Cycles         cycle 1,067,016.50
    Total SM Elapsed Cycles          cycle   46,213,388
    Average SMSP Active Cycles       cycle 1,066,755.68
    Total SMSP Elapsed Cycles        cycle  184,853,552
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.281%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 7.88% above the average, while the minimum instance value is 5.42% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.101%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 7.69% above the average, while the minimum instance value is 5.39% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.281%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 7.88% above the average, while the minimum instance value is 5.42% below the        
          average.                                                                                                      

  void cub::DeviceRadixSortExclusiveSumKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, unsigned long long>(T2 *) (4, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.83
    SM Frequency                    Ghz         1.09
    Elapsed Cycles                cycle        3,691
    Memory Throughput                 %         1.88
    DRAM Throughput                   %         0.95
    Duration                         us         3.39
    L1/TEX Cache Throughput           %         9.04
    L2 Cache Throughput               %         1.88
    SM Active Cycles              cycle       183.68
    Compute (SM) Throughput           %         0.40
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      4
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.34
    # SMs                                         SM              40
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 90%                                                                                             
          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.47
    Achieved Active Warps Per SM           warp         7.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          220
    Total DRAM Elapsed Cycles        cycle      185,344
    Average L1 Active Cycles         cycle       183.68
    Total L1 Elapsed Cycles          cycle      147,452
    Average L2 Active Cycles         cycle       353.41
    Total L2 Elapsed Cycles          cycle      113,248
    Average SM Active Cycles         cycle       183.68
    Total SM Elapsed Cycles          cycle      147,452
    Average SMSP Active Cycles       cycle       180.18
    Total SMSP Elapsed Cycles        cycle      589,808
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.223%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 52.31% above the average, while the minimum instance value is 89.53% below the      
          average.                                                                                                      

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,561,090
    Memory Throughput                 %        92.39
    DRAM Throughput                   %        92.39
    Duration                         ms         4.11
    L1/TEX Cache Throughput           %        47.45
    L2 Cache Throughput               %        42.16
    SM Active Cycles              cycle 4,538,473.10
    Compute (SM) Throughput           %        47.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        47.76
    Achieved Active Warps Per SM           warp        22.93
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,555,472
    Total DRAM Elapsed Cycles        cycle  229,950,464
    Average L1 Active Cycles         cycle 4,538,473.10
    Total L1 Elapsed Cycles          cycle  182,003,146
    Average L2 Active Cycles         cycle 4,352,723.12
    Total L2 Elapsed Cycles          cycle  140,056,032
    Average SM Active Cycles         cycle 4,538,473.10
    Total SM Elapsed Cycles          cycle  182,003,146
    Average SMSP Active Cycles       cycle 4,540,043.46
    Total SMSP Elapsed Cycles        cycle  728,012,584
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,559,013
    Memory Throughput                 %        92.56
    DRAM Throughput                   %        92.56
    Duration                         ms         4.11
    L1/TEX Cache Throughput           %        47.54
    L2 Cache Throughput               %        42.30
    SM Active Cycles              cycle 4,550,775.78
    Compute (SM) Throughput           %        47.38
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        48.25
    Achieved Active Warps Per SM           warp        23.16
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,587,490
    Total DRAM Elapsed Cycles        cycle  229,806,080
    Average L1 Active Cycles         cycle 4,550,775.78
    Total L1 Elapsed Cycles          cycle  182,620,862
    Average L2 Active Cycles         cycle 4,360,688.38
    Total L2 Elapsed Cycles          cycle  139,974,400
    Average SM Active Cycles         cycle 4,550,775.78
    Total SM Elapsed Cycles          cycle  182,620,862
    Average SMSP Active Cycles       cycle 4,548,718.38
    Total SMSP Elapsed Cycles        cycle  730,483,448
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,557,291
    Memory Throughput                 %        92.46
    DRAM Throughput                   %        92.46
    Duration                         ms         4.11
    L1/TEX Cache Throughput           %        47.42
    L2 Cache Throughput               %        42.21
    SM Active Cycles              cycle 4,548,684.03
    Compute (SM) Throughput           %        47.48
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        47.60
    Achieved Active Warps Per SM           warp        22.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,550,588
    Total DRAM Elapsed Cycles        cycle  229,718,016
    Average L1 Active Cycles         cycle 4,548,684.03
    Total L1 Elapsed Cycles          cycle  182,052,038
    Average L2 Active Cycles         cycle 4,354,829.09
    Total L2 Elapsed Cycles          cycle  139,921,440
    Average SM Active Cycles         cycle 4,548,684.03
    Total SM Elapsed Cycles          cycle  182,052,038
    Average SMSP Active Cycles       cycle 4,543,799.94
    Total SMSP Elapsed Cycles        cycle  728,208,152
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,563,176
    Memory Throughput                 %        92.49
    DRAM Throughput                   %        92.49
    Duration                         ms         4.11
    L1/TEX Cache Throughput           %        47.72
    L2 Cache Throughput               %        42.20
    SM Active Cycles              cycle 4,537,913.38
    Compute (SM) Throughput           %        47.52
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        48.13
    Achieved Active Warps Per SM           warp        23.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,592,346
    Total DRAM Elapsed Cycles        cycle  230,016,000
    Average L1 Active Cycles         cycle 4,537,913.38
    Total L1 Elapsed Cycles          cycle  182,228,168
    Average L2 Active Cycles         cycle 4,356,135.94
    Total L2 Elapsed Cycles          cycle  140,102,272
    Average SM Active Cycles         cycle 4,537,913.38
    Total SM Elapsed Cycles          cycle  182,228,168
    Average SMSP Active Cycles       cycle 4,550,116.51
    Total SMSP Elapsed Cycles        cycle  728,912,672
    -------------------------- ----------- ------------

  update_ranks_kernel(const int *, const int *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle    66,762,384
    Memory Throughput                 %         43.26
    DRAM Throughput                   %         28.35
    Duration                         ms         60.15
    L1/TEX Cache Throughput           %         25.64
    L2 Cache Throughput               %         43.26
    SM Active Cycles              cycle 66,725,927.45
    Compute (SM) Throughput           %          1.96
    ----------------------- ----------- -------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.73
    Achieved Active Warps Per SM           warp        38.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.27%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle    119,245,228
    Total DRAM Elapsed Cycles        cycle  3,365,307,392
    Average L1 Active Cycles         cycle  66,725,927.45
    Total L1 Elapsed Cycles          cycle  2,668,163,838
    Average L2 Active Cycles         cycle  64,031,100.28
    Total L2 Elapsed Cycles          cycle  2,049,785,760
    Average SM Active Cycles         cycle  66,725,927.45
    Total SM Elapsed Cycles          cycle  2,668,163,838
    Average SMSP Active Cycles       cycle  66,725,755.83
    Total SMSP Elapsed Cycles        cycle 10,672,655,352
    -------------------------- ----------- --------------

  prefix_sum_kernel(int *, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle 3,972,759,063
    Memory Throughput                 %          0.26
    DRAM Throughput                   %          0.06
    Duration                          s          3.58
    L1/TEX Cache Throughput           %         10.56
    L2 Cache Throughput               %          0.08
    SM Active Cycles              cycle 99,318,770.78
    Compute (SM) Throughput           %          0.26
    ----------------------- ----------- -------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread               1
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 96.88%                                                                                          
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp            1
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.75%                                                                                    
          The difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (33.3%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ---------------
    Metric Name                Metric Unit    Metric Value
    -------------------------- ----------- ---------------
    Average DRAM Active Cycles       cycle      14,068,644
    Total DRAM Elapsed Cycles        cycle 200,255,745,024
    Average L1 Active Cycles         cycle   99,318,770.78
    Total L1 Elapsed Cycles          cycle 158,905,439,418
    Average L2 Active Cycles         cycle   76,332,125.06
    Total L2 Elapsed Cycles          cycle 121,974,440,384
    Average SM Active Cycles         cycle   99,318,770.78
    Total SM Elapsed Cycles          cycle 158,905,439,418
    Average SMSP Active Cycles       cycle   24,830,265.19
    Total SMSP Elapsed Cycles        cycle 635,621,757,672
    -------------------------- ----------- ---------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    2,263,298
    Memory Throughput                 %        91.85
    DRAM Throughput                   %        91.85
    Duration                         ms         2.04
    L1/TEX Cache Throughput           %        34.39
    L2 Cache Throughput               %        38.86
    SM Active Cycles              cycle    2,258,159
    Compute (SM) Throughput           %        28.99
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.21
    Achieved Active Warps Per SM           warp        37.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.79%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   13,098,440
    Total DRAM Elapsed Cycles        cycle  114,086,912
    Average L1 Active Cycles         cycle    2,258,159
    Total L1 Elapsed Cycles          cycle   90,418,336
    Average L2 Active Cycles         cycle 2,166,066.44
    Total L2 Elapsed Cycles          cycle   69,489,280
    Average SM Active Cycles         cycle    2,258,159
    Total SM Elapsed Cycles          cycle   90,418,336
    Average SMSP Active Cycles       cycle 2,257,259.41
    Total SMSP Elapsed Cycles        cycle  361,673,344
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortHistogramKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, unsigned long long, detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5) (400, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,164,994
    Memory Throughput                 %        89.60
    DRAM Throughput                   %        89.60
    Duration                         ms         1.05
    L1/TEX Cache Throughput           %        76.94
    L2 Cache Throughput               %        43.00
    SM Active Cycles              cycle 1,067,370.68
    Compute (SM) Throughput           %        71.10
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    400
    Registers Per Thread             register/thread              47
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            4.10
    # SMs                                         SM              40
    Threads                                   thread          51,200
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        74.50
    Achieved Active Warps Per SM           warp        35.76
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    6,578,106
    Total DRAM Elapsed Cycles        cycle   58,730,496
    Average L1 Active Cycles         cycle 1,067,370.68
    Total L1 Elapsed Cycles          cycle   46,202,906
    Average L2 Active Cycles         cycle 1,097,352.56
    Total L2 Elapsed Cycles          cycle   35,768,320
    Average SM Active Cycles         cycle 1,067,370.68
    Total SM Elapsed Cycles          cycle   46,202,906
    Average SMSP Active Cycles       cycle 1,066,561.60
    Total SMSP Elapsed Cycles        cycle  184,811,624
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.904%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 7.47% above the average, while the minimum instance value is 5.28% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.339%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 7.95% above the average, while the minimum instance value is 5.31% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.904%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 7.47% above the average, while the minimum instance value is 5.28% below the        
          average.                                                                                                      

  void cub::DeviceRadixSortExclusiveSumKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, unsigned long long>(T2 *) (4, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.90
    SM Frequency                    Ghz         1.09
    Elapsed Cycles                cycle        3,676
    Memory Throughput                 %         1.75
    DRAM Throughput                   %         1.23
    Duration                         us         3.36
    L1/TEX Cache Throughput           %         8.49
    L2 Cache Throughput               %         1.75
    SM Active Cycles              cycle       195.47
    Compute (SM) Throughput           %         0.39
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      4
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.34
    # SMs                                         SM              40
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 90%                                                                                             
          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.49
    Achieved Active Warps Per SM           warp         7.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.51%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          286
    Total DRAM Elapsed Cycles        cycle      185,344
    Average L1 Active Cycles         cycle       195.47
    Total L1 Elapsed Cycles          cycle      147,880
    Average L2 Active Cycles         cycle       349.19
    Total L2 Elapsed Cycles          cycle      112,864
    Average SM Active Cycles         cycle       195.47
    Total SM Elapsed Cycles          cycle      147,880
    Average SMSP Active Cycles       cycle          182
    Total SMSP Elapsed Cycles        cycle      591,520
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.491%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 55.46% above the average, while the minimum instance value is 89.40% below the      
          average.                                                                                                      

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,849,410
    Memory Throughput                 %        87.38
    DRAM Throughput                   %        87.38
    Duration                         ms         4.37
    L1/TEX Cache Throughput           %        64.50
    L2 Cache Throughput               %        44.22
    SM Active Cycles              cycle 4,824,473.10
    Compute (SM) Throughput           %        64.31
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.36
    Achieved Active Warps Per SM           warp        23.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,698,630
    Total DRAM Elapsed Cycles        cycle  244,443,136
    Average L1 Active Cycles         cycle 4,824,473.10
    Total L1 Elapsed Cycles          cycle  193,528,174
    Average L2 Active Cycles         cycle 4,617,546.94
    Total L2 Elapsed Cycles          cycle  148,889,792
    Average SM Active Cycles         cycle 4,824,473.10
    Total SM Elapsed Cycles          cycle  193,528,174
    Average SMSP Active Cycles       cycle 4,847,809.20
    Total SMSP Elapsed Cycles        cycle  774,112,696
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,842,200
    Memory Throughput                 %        87.51
    DRAM Throughput                   %        87.51
    Duration                         ms         4.36
    L1/TEX Cache Throughput           %        64.43
    L2 Cache Throughput               %        44.27
    SM Active Cycles              cycle 4,828,766.03
    Compute (SM) Throughput           %        63.95
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.36
    Achieved Active Warps Per SM           warp        23.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,698,800
    Total DRAM Elapsed Cycles        cycle  244,080,640
    Average L1 Active Cycles         cycle 4,828,766.03
    Total L1 Elapsed Cycles          cycle  194,495,622
    Average L2 Active Cycles         cycle 4,636,950.25
    Total L2 Elapsed Cycles          cycle  148,668,352
    Average SM Active Cycles         cycle 4,828,766.03
    Total SM Elapsed Cycles          cycle  194,495,622
    Average SMSP Active Cycles       cycle 4,840,002.68
    Total SMSP Elapsed Cycles        cycle  777,982,488
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,585,171
    Memory Throughput                 %        92.48
    DRAM Throughput                   %        92.48
    Duration                         ms         4.13
    L1/TEX Cache Throughput           %        68.08
    L2 Cache Throughput               %        42.22
    SM Active Cycles              cycle 4,571,128.62
    Compute (SM) Throughput           %        67.92
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.35
    Achieved Active Warps Per SM           warp        23.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,717,990
    Total DRAM Elapsed Cycles        cycle  231,134,208
    Average L1 Active Cycles         cycle 4,571,128.62
    Total L1 Elapsed Cycles          cycle  183,246,140
    Average L2 Active Cycles         cycle 4,381,306.94
    Total L2 Elapsed Cycles          cycle  140,779,072
    Average SM Active Cycles         cycle 4,571,128.62
    Total SM Elapsed Cycles          cycle  183,246,140
    Average SMSP Active Cycles       cycle 4,570,765.84
    Total SMSP Elapsed Cycles        cycle  732,984,560
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,532,701
    Memory Throughput                 %        93.46
    DRAM Throughput                   %        93.46
    Duration                         ms         4.08
    L1/TEX Cache Throughput           %        68.85
    L2 Cache Throughput               %        42.63
    SM Active Cycles              cycle 4,525,142.25
    Compute (SM) Throughput           %        68.80
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.37
    Achieved Active Warps Per SM           warp        23.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,693,066
    Total DRAM Elapsed Cycles        cycle  228,478,976
    Average L1 Active Cycles         cycle 4,525,142.25
    Total L1 Elapsed Cycles          cycle  181,186,542
    Average L2 Active Cycles         cycle 4,335,057.91
    Total L2 Elapsed Cycles          cycle  139,166,368
    Average SM Active Cycles         cycle 4,525,142.25
    Total SM Elapsed Cycles          cycle  181,186,542
    Average SMSP Active Cycles       cycle 4,523,102.61
    Total SMSP Elapsed Cycles        cycle  724,746,168
    -------------------------- ----------- ------------

  update_ranks_kernel(const int *, const int *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle    66,748,314
    Memory Throughput                 %         43.50
    DRAM Throughput                   %         28.35
    Duration                         ms         60.13
    L1/TEX Cache Throughput           %         25.62
    L2 Cache Throughput               %         43.50
    SM Active Cycles              cycle 66,746,927.48
    Compute (SM) Throughput           %          1.96
    ----------------------- ----------- -------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.37
    Achieved Active Warps Per SM           warp        38.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle    119,237,462
    Total DRAM Elapsed Cycles        cycle  3,364,597,760
    Average L1 Active Cycles         cycle  66,746,927.48
    Total L1 Elapsed Cycles          cycle  2,669,690,314
    Average L2 Active Cycles         cycle  64,063,439.03
    Total L2 Elapsed Cycles          cycle  2,049,353,536
    Average SM Active Cycles         cycle  66,746,927.48
    Total SM Elapsed Cycles          cycle  2,669,690,314
    Average SMSP Active Cycles       cycle  66,760,473.96
    Total SMSP Elapsed Cycles        cycle 10,678,761,256
    -------------------------- ----------- --------------

  prefix_sum_kernel(int *, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle 3,972,761,949
    Memory Throughput                 %          0.26
    DRAM Throughput                   %          0.06
    Duration                          s          3.58
    L1/TEX Cache Throughput           %         10.56
    L2 Cache Throughput               %          0.08
    SM Active Cycles              cycle 99,316,850.38
    Compute (SM) Throughput           %          0.26
    ----------------------- ----------- -------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread               1
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 96.88%                                                                                          
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp            1
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.75%                                                                                    
          The difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (33.3%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ---------------
    Metric Name                Metric Unit    Metric Value
    -------------------------- ----------- ---------------
    Average DRAM Active Cycles       cycle      14,078,532
    Total DRAM Elapsed Cycles        cycle 200,255,890,432
    Average L1 Active Cycles         cycle   99,316,850.38
    Total L1 Elapsed Cycles          cycle 158,904,675,650
    Average L2 Active Cycles         cycle      76,315,187
    Total L2 Elapsed Cycles          cycle 121,974,528,736
    Average SM Active Cycles         cycle   99,316,850.38
    Total SM Elapsed Cycles          cycle 158,904,675,650
    Average SMSP Active Cycles       cycle   24,829,079.08
    Total SMSP Elapsed Cycles        cycle 635,618,702,600
    -------------------------- ----------- ---------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    2,262,583
    Memory Throughput                 %        91.88
    DRAM Throughput                   %        91.88
    Duration                         ms         2.04
    L1/TEX Cache Throughput           %        33.80
    L2 Cache Throughput               %        39.99
    SM Active Cycles              cycle 2,257,491.05
    Compute (SM) Throughput           %        28.99
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.22
    Achieved Active Warps Per SM           warp        37.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.78%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   13,098,650
    Total DRAM Elapsed Cycles        cycle  114,051,072
    Average L1 Active Cycles         cycle 2,257,491.05
    Total L1 Elapsed Cycles          cycle   90,424,738
    Average L2 Active Cycles         cycle 2,165,682.41
    Total L2 Elapsed Cycles          cycle   69,467,360
    Average SM Active Cycles         cycle 2,257,491.05
    Total SM Elapsed Cycles          cycle   90,424,738
    Average SMSP Active Cycles       cycle 2,257,320.06
    Total SMSP Elapsed Cycles        cycle  361,698,952
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortHistogramKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, unsigned long long, detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5) (400, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,156,434
    Memory Throughput                 %        90.27
    DRAM Throughput                   %        90.27
    Duration                         ms         1.04
    L1/TEX Cache Throughput           %        76.97
    L2 Cache Throughput               %        43.48
    SM Active Cycles              cycle 1,066,748.52
    Compute (SM) Throughput           %        71.16
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    400
    Registers Per Thread             register/thread              47
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            4.10
    # SMs                                         SM              40
    Threads                                   thread          51,200
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        74.66
    Achieved Active Warps Per SM           warp        35.84
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    6,578,390
    Total DRAM Elapsed Cycles        cycle   58,297,344
    Average L1 Active Cycles         cycle 1,066,748.52
    Total L1 Elapsed Cycles          cycle   46,155,886
    Average L2 Active Cycles         cycle 1,099,861.78
    Total L2 Elapsed Cycles          cycle   35,505,760
    Average SM Active Cycles         cycle 1,066,748.52
    Total SM Elapsed Cycles          cycle   46,155,886
    Average SMSP Active Cycles       cycle 1,067,402.27
    Total SMSP Elapsed Cycles        cycle  184,623,544
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.802%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 7.36% above the average, while the minimum instance value is 5.19% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.997%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 7.56% above the average, while the minimum instance value is 5.27% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.802%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 7.36% above the average, while the minimum instance value is 5.19% below the        
          average.                                                                                                      

  void cub::DeviceRadixSortExclusiveSumKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, unsigned long long>(T2 *) (4, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.87
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle        3,613
    Memory Throughput                 %         1.88
    DRAM Throughput                   %         0.96
    Duration                         us         3.30
    L1/TEX Cache Throughput           %         8.82
    L2 Cache Throughput               %         1.88
    SM Active Cycles              cycle       188.12
    Compute (SM) Throughput           %         0.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      4
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.34
    # SMs                                         SM              40
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 90%                                                                                             
          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.48
    Achieved Active Warps Per SM           warp         7.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          218
    Total DRAM Elapsed Cycles        cycle      181,248
    Average L1 Active Cycles         cycle       188.12
    Total L1 Elapsed Cycles          cycle      142,212
    Average L2 Active Cycles         cycle       335.69
    Total L2 Elapsed Cycles          cycle      110,912
    Average SM Active Cycles         cycle       188.12
    Total SM Elapsed Cycles          cycle      142,212
    Average SMSP Active Cycles       cycle       186.80
    Total SMSP Elapsed Cycles        cycle      568,848
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.119%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 52.85% above the average, while the minimum instance value is 88.98% below the      
          average.                                                                                                      

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,706,775
    Memory Throughput                 %        89.94
    DRAM Throughput                   %        89.94
    Duration                         ms         4.24
    L1/TEX Cache Throughput           %        66.15
    L2 Cache Throughput               %        45.72
    SM Active Cycles              cycle 4,705,501.58
    Compute (SM) Throughput           %        66.19
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.35
    Achieved Active Warps Per SM           warp        23.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,672,974
    Total DRAM Elapsed Cycles        cycle  237,252,608
    Average L1 Active Cycles         cycle 4,705,501.58
    Total L1 Elapsed Cycles          cycle  188,053,014
    Average L2 Active Cycles         cycle 4,491,045.44
    Total L2 Elapsed Cycles          cycle  144,510,528
    Average SM Active Cycles         cycle 4,705,501.58
    Total SM Elapsed Cycles          cycle  188,053,014
    Average SMSP Active Cycles       cycle 4,685,138.69
    Total SMSP Elapsed Cycles        cycle  752,212,056
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,691,476
    Memory Throughput                 %        90.22
    DRAM Throughput                   %        90.22
    Duration                         ms         4.23
    L1/TEX Cache Throughput           %        66.49
    L2 Cache Throughput               %        45.86
    SM Active Cycles              cycle 4,679,353.78
    Compute (SM) Throughput           %        66.48
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.35
    Achieved Active Warps Per SM           warp        23.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,670,604
    Total DRAM Elapsed Cycles        cycle  236,482,560
    Average L1 Active Cycles         cycle 4,679,353.78
    Total L1 Elapsed Cycles          cycle  187,264,476
    Average L2 Active Cycles         cycle 4,499,372.53
    Total L2 Elapsed Cycles          cycle  144,041,408
    Average SM Active Cycles         cycle 4,679,353.78
    Total SM Elapsed Cycles          cycle  187,264,476
    Average SMSP Active Cycles       cycle 4,675,710.17
    Total SMSP Elapsed Cycles        cycle  749,057,904
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,514,167
    Memory Throughput                 %        93.84
    DRAM Throughput                   %        93.84
    Duration                         ms         4.07
    L1/TEX Cache Throughput           %        69.07
    L2 Cache Throughput               %        42.77
    SM Active Cycles              cycle 4,510,208.75
    Compute (SM) Throughput           %        68.87
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.37
    Achieved Active Warps Per SM           warp        23.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,690,676
    Total DRAM Elapsed Cycles        cycle  227,551,232
    Average L1 Active Cycles         cycle 4,510,208.75
    Total L1 Elapsed Cycles          cycle  180,880,230
    Average L2 Active Cycles         cycle 4,320,111.66
    Total L2 Elapsed Cycles          cycle  138,596,736
    Average SM Active Cycles         cycle 4,510,208.75
    Total SM Elapsed Cycles          cycle  180,880,230
    Average SMSP Active Cycles       cycle 4,508,775.69
    Total SMSP Elapsed Cycles        cycle  723,520,920
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,516,514
    Memory Throughput                 %        93.74
    DRAM Throughput                   %        93.74
    Duration                         ms         4.07
    L1/TEX Cache Throughput           %        69.15
    L2 Cache Throughput               %        42.57
    SM Active Cycles              cycle 4,502,793.50
    Compute (SM) Throughput           %        69.11
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.37
    Achieved Active Warps Per SM           warp        23.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,677,476
    Total DRAM Elapsed Cycles        cycle  227,675,136
    Average L1 Active Cycles         cycle 4,502,793.50
    Total L1 Elapsed Cycles          cycle  180,303,922
    Average L2 Active Cycles         cycle 4,317,895.41
    Total L2 Elapsed Cycles          cycle  138,672,640
    Average SM Active Cycles         cycle 4,502,793.50
    Total SM Elapsed Cycles          cycle  180,303,922
    Average SMSP Active Cycles       cycle 4,500,986.54
    Total SMSP Elapsed Cycles        cycle  721,215,688
    -------------------------- ----------- ------------

  update_ranks_kernel(const int *, const int *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle    66,772,248
    Memory Throughput                 %         43.53
    DRAM Throughput                   %         28.34
    Duration                         ms         60.16
    L1/TEX Cache Throughput           %         25.61
    L2 Cache Throughput               %         43.53
    SM Active Cycles              cycle 66,781,454.90
    Compute (SM) Throughput           %          1.96
    ----------------------- ----------- -------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.21
    Achieved Active Warps Per SM           warp        38.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.79%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle    119,236,690
    Total DRAM Elapsed Cycles        cycle  3,365,803,008
    Average L1 Active Cycles         cycle  66,781,454.90
    Total L1 Elapsed Cycles          cycle  2,671,630,556
    Average L2 Active Cycles         cycle  64,065,252.50
    Total L2 Elapsed Cycles          cycle  2,050,088,256
    Average SM Active Cycles         cycle  66,781,454.90
    Total SM Elapsed Cycles          cycle  2,671,630,556
    Average SMSP Active Cycles       cycle  66,802,452.47
    Total SMSP Elapsed Cycles        cycle 10,686,522,224
    -------------------------- ----------- --------------

  prefix_sum_kernel(int *, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle 3,972,779,414
    Memory Throughput                 %          0.26
    DRAM Throughput                   %          0.06
    Duration                          s          3.58
    L1/TEX Cache Throughput           %         10.56
    L2 Cache Throughput               %          0.08
    SM Active Cycles              cycle 99,320,284.45
    Compute (SM) Throughput           %          0.26
    ----------------------- ----------- -------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread               1
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 96.88%                                                                                          
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp            1
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.75%                                                                                    
          The difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (33.3%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ---------------
    Metric Name                Metric Unit    Metric Value
    -------------------------- ----------- ---------------
    Average DRAM Active Cycles       cycle      14,074,690
    Total DRAM Elapsed Cycles        cycle 200,256,772,096
    Average L1 Active Cycles         cycle   99,320,284.45
    Total L1 Elapsed Cycles          cycle 158,910,303,116
    Average L2 Active Cycles         cycle   76,291,298.69
    Total L2 Elapsed Cycles          cycle 121,975,065,344
    Average SM Active Cycles         cycle   99,320,284.45
    Total SM Elapsed Cycles          cycle 158,910,303,116
    Average SMSP Active Cycles       cycle   24,829,291.54
    Total SMSP Elapsed Cycles        cycle 635,641,212,464
    -------------------------- ----------- ---------------

  compute_rank_kernel(const int *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle    34,187,441
    Memory Throughput                 %         42.55
    DRAM Throughput                   %         27.36
    Duration                         ms         30.80
    L1/TEX Cache Throughput           %         31.03
    L2 Cache Throughput               %         42.55
    SM Active Cycles              cycle 34,196,798.92
    Compute (SM) Throughput           %          2.55
    ----------------------- ----------- -------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.55
    Achieved Active Warps Per SM           warp        38.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle    58,946,940
    Total DRAM Elapsed Cycles        cycle 1,723,293,696
    Average L1 Active Cycles         cycle 34,196,798.92
    Total L1 Elapsed Cycles          cycle 1,367,096,982
    Average L2 Active Cycles         cycle 32,581,974.81
    Total L2 Elapsed Cycles          cycle 1,049,646,592
    Average SM Active Cycles         cycle 34,196,798.92
    Total SM Elapsed Cycles          cycle 1,367,096,982
    Average SMSP Active Cycles       cycle 34,171,565.31
    Total SMSP Elapsed Cycles        cycle 5,468,387,928
    -------------------------- ----------- -------------

  compute_lcp_kernel(const unsigned char *, const int *, const int *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle    57,609,961
    Memory Throughput                 %         41.85
    DRAM Throughput                   %         32.91
    Duration                         ms         51.90
    L1/TEX Cache Throughput           %         19.80
    L2 Cache Throughput               %         41.85
    SM Active Cycles              cycle 57,596,992.48
    Compute (SM) Throughput           %          2.35
    ----------------------- ----------- -------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.81
    Achieved Active Warps Per SM           warp        42.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle   119,454,062
    Total DRAM Elapsed Cycles        cycle 2,903,957,504
    Average L1 Active Cycles         cycle 57,596,992.48
    Total L1 Elapsed Cycles          cycle 2,304,025,598
    Average L2 Active Cycles         cycle 55,262,765.78
    Total L2 Elapsed Cycles          cycle 1,768,781,344
    Average SM Active Cycles         cycle 57,596,992.48
    Total SM Elapsed Cycles          cycle 2,304,025,598
    Average SMSP Active Cycles       cycle 57,599,336.81
    Total SMSP Elapsed Cycles        cycle 9,216,102,392
    -------------------------- ----------- -------------

  void core::_kernel_agent<__reduce::DrainAgent<int>, cub::GridQueue<unsigned int>, int>(T2...) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.74
    SM Frequency                    Ghz         1.07
    Elapsed Cycles                cycle        2,611
    Memory Throughput                 %         1.13
    DRAM Throughput                   %         0.26
    Duration                         us         2.43
    L1/TEX Cache Throughput           %        55.81
    L2 Cache Throughput               %         1.13
    SM Active Cycles              cycle        21.50
    Compute (SM) Throughput           %         0.00
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread               1
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 96.88%                                                                                          
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block          128
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp            1
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.75%                                                                                    
          The difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (33.3%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           42
    Total DRAM Elapsed Cycles        cycle      131,072
    Average L1 Active Cycles         cycle        21.50
    Total L1 Elapsed Cycles          cycle      126,300
    Average L2 Active Cycles         cycle       179.69
    Total L2 Elapsed Cycles          cycle       80,192
    Average SM Active Cycles         cycle        21.50
    Total SM Elapsed Cycles          cycle      126,300
    Average SMSP Active Cycles       cycle         5.27
    Total SMSP Elapsed Cycles        cycle      505,200
    -------------------------- ----------- ------------

  void core::_kernel_agent<__reduce::ReduceAgent<thrust::zip_iterator<cuda::tuple<thrust::device_ptr<int>, thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>>>, cuda::tuple<int, long> *, cuda::tuple<int, long>, int, __extrema::arg_max_f<int, long, thrust::less<int>>>, thrust::zip_iterator<cuda::tuple<thrust::device_ptr<int>, thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>>>, cuda::tuple<int, long> *, int, cub::GridEvenShare<int>, cub::GridQueue<unsigned int>, __extrema::arg_max_f<int, long, thrust::less<int>>>(T2...) (240, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,080,256
    Memory Throughput                 %        96.65
    DRAM Throughput                   %        96.65
    Duration                         us       973.22
    L1/TEX Cache Throughput           %        30.73
    L2 Cache Throughput               %        39.79
    SM Active Cycles              cycle 1,073,064.20
    Compute (SM) Throughput           %        33.42
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    240
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread          61,440
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.85
    Achieved Active Warps Per SM           warp        47.93
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    6,578,276
    Total DRAM Elapsed Cycles        cycle   54,450,176
    Average L1 Active Cycles         cycle 1,073,064.20
    Total L1 Elapsed Cycles          cycle   43,022,174
    Average L2 Active Cycles         cycle 1,025,106.78
    Total L2 Elapsed Cycles          cycle   33,167,168
    Average SM Active Cycles         cycle 1,073,064.20
    Total SM Elapsed Cycles          cycle   43,022,174
    Average SMSP Active Cycles       cycle 1,072,328.27
    Total SMSP Elapsed Cycles        cycle  172,088,696
    -------------------------- ----------- ------------

  void core::_kernel_agent<__reduce::ReduceAgent<cuda::tuple<int, long> *, cuda::tuple<int, long> *, cuda::tuple<int, long>, int, __extrema::arg_max_f<int, long, thrust::less<int>>>, cuda::tuple<int, long> *, cuda::tuple<int, long> *, int, __extrema::arg_max_f<int, long, thrust::less<int>>>(T2...) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.89
    SM Frequency                    Ghz         1.09
    Elapsed Cycles                cycle        5,535
    Memory Throughput                 %         3.43
    DRAM Throughput                   %         1.65
    Duration                         us         5.06
    L1/TEX Cache Throughput           %        12.61
    L2 Cache Throughput               %         3.43
    SM Active Cycles              cycle        95.33
    Compute (SM) Throughput           %         0.18
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread             256
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.32
    Achieved Active Warps Per SM           warp         6.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          576
    Total DRAM Elapsed Cycles        cycle      278,528
    Average L1 Active Cycles         cycle        95.33
    Total L1 Elapsed Cycles          cycle      220,852
    Average L2 Active Cycles         cycle       555.84
    Total L2 Elapsed Cycles          cycle      169,792
    Average SM Active Cycles         cycle        95.33
    Total SM Elapsed Cycles          cycle      220,852
    Average SMSP Active Cycles       cycle        82.99
    Total SMSP Elapsed Cycles        cycle      883,408
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.013%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 47.86% above the average, while the minimum instance value is 40.45% below the      
          average.                                                                                                      

