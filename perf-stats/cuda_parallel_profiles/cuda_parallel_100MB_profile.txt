==PROF== Connected to process 487361 (/home/emo/Documents/Programmazione/progetti/HPC/Manber-Myers-Parallelization/cmake-build-debug-cuda-parallel/hpc_cuda_parallel)
==PROF== Profiling "static_kernel" - 0: 0%....50%....100% - 8 passes
==PROF== Profiling "init_ranks_kernel" - 1: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 2: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 3: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 4: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 5: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 6: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 7: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 8: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 9: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortHistogramKernel" - 10: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortExclusiveSumKe..." - 11: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 12: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 13: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 14: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 15: 0%....50%....100% - 8 passes
==PROF== Profiling "update_ranks_kernel" - 16: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceScanInitKernel" - 17: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceScanKernel" - 18: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 19: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 20: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 21: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 22: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 23: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 24: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 25: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 26: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortHistogramKernel" - 27: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortExclusiveSumKe..." - 28: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 29: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 30: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 31: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 32: 0%....50%....100% - 8 passes
==PROF== Profiling "update_ranks_kernel" - 33: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceScanInitKernel" - 34: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceScanKernel" - 35: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 36: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 37: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 38: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 39: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 40: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 41: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 42: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 43: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortHistogramKernel" - 44: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortExclusiveSumKe..." - 45: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 46: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 47: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 48: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 49: 0%....50%....100% - 8 passes
==PROF== Profiling "update_ranks_kernel" - 50: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceScanInitKernel" - 51: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceScanKernel" - 52: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 53: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 54: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 55: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 56: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 57: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 58: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 59: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 60: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortHistogramKernel" - 61: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortExclusiveSumKe..." - 62: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 63: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 64: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 65: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 66: 0%....50%....100% - 8 passes
==PROF== Profiling "update_ranks_kernel" - 67: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceScanInitKernel" - 68: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceScanKernel" - 69: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 70: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 71: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 72: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 73: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 74: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 75: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 76: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_pairs_kernel" - 77: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortHistogramKernel" - 78: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortExclusiveSumKe..." - 79: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 80: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 81: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 82: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceRadixSortOnesweepKernel" - 83: 0%....50%....100% - 8 passes
==PROF== Profiling "update_ranks_kernel" - 84: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceScanInitKernel" - 85: 0%....50%....100% - 8 passes
==PROF== Profiling "DeviceScanKernel" - 86: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_rank_kernel" - 87: 0%....50%....100% - 8 passes
==PROF== Profiling "compute_lcp_kernel" - 88: 0%....50%....100% - 8 passes
==PROF== Profiling "_kernel_agent" - 89: 0%....50%....100% - 8 passes
==PROF== Profiling "_kernel_agent" - 90: 0%....50%....100% - 8 passes
==PROF== Profiling "_kernel_agent" - 91: 0%....50%....100% - 8 passes
=== CUDA PARALLEL SA + LCP ===
size=100 MB  streams=8   time_build=28.3509 s
max_lrs_len=4   pos=36741996
Found 1 CUDA device(s):
  [0] NVIDIA GeForce RTX 3070 Laptop GPU - 7840 MB global mem, 40 SMs
==PROF== Disconnected from process 487361
[487361] hpc_cuda_parallel@127.0.0.1
  void for_each::static_kernel<for_each::policy_350_t, long, __tabulate::functor<thrust::device_ptr<int>, system::compute_sequence_value<int, void>, long>>(T2, T3) (204800, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,072,379
    Memory Throughput                 %        96.88
    DRAM Throughput                   %        96.88
    Duration                         us       966.24
    L1/TEX Cache Throughput           %        61.20
    L2 Cache Throughput               %        40.98
    SM Active Cycles              cycle 1,066,742.48
    Compute (SM) Throughput           %        22.95
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                204,800
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      52,428,800
    Uses Green Context                                             0
    Waves Per SM                                              853.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        66.12
    Achieved Active Warps Per SM           warp        31.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 33.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (66.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    6,546,860
    Total DRAM Elapsed Cycles        cycle   54,059,008
    Average L1 Active Cycles         cycle 1,066,742.48
    Total L1 Elapsed Cycles          cycle   42,836,092
    Average L2 Active Cycles         cycle   992,268.16
    Total L2 Elapsed Cycles          cycle   31,998,944
    Average SM Active Cycles         cycle 1,066,742.48
    Total SM Elapsed Cycles          cycle   42,836,092
    Average SMSP Active Cycles       cycle 1,066,088.89
    Total SMSP Elapsed Cycles        cycle  171,344,368
    -------------------------- ----------- ------------

  init_ranks_kernel(const unsigned char *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,628,768
    Memory Throughput                 %        79.73
    DRAM Throughput                   %        79.73
    Duration                         ms         1.47
    L1/TEX Cache Throughput           %        43.30
    L2 Cache Throughput               %        33.72
    SM Active Cycles              cycle 1,623,087.48
    Compute (SM) Throughput           %        30.20
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.31
    Achieved Active Warps Per SM           warp        36.63
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.69%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    8,183,158
    Total DRAM Elapsed Cycles        cycle   82,104,320
    Average L1 Active Cycles         cycle 1,623,087.48
    Total L1 Elapsed Cycles          cycle   65,106,928
    Average L2 Active Cycles         cycle 1,494,489.62
    Total L2 Elapsed Cycles          cycle   48,600,480
    Average SM Active Cycles         cycle 1,623,087.48
    Total SM Elapsed Cycles          cycle   65,106,928
    Average SMSP Active Cycles       cycle 1,622,024.77
    Total SMSP Elapsed Cycles        cycle  260,427,712
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      285,072
    Memory Throughput                 %        90.73
    DRAM Throughput                   %        90.73
    Duration                         us       256.96
    L1/TEX Cache Throughput           %        34.43
    L2 Cache Throughput               %        39.13
    SM Active Cycles              cycle   280,421.22
    Compute (SM) Throughput           %        29.01
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.86
    Achieved Active Warps Per SM           warp        37.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,630,358
    Total DRAM Elapsed Cycles        cycle   14,374,912
    Average L1 Active Cycles         cycle   280,421.22
    Total L1 Elapsed Cycles          cycle   11,295,450
    Average L2 Active Cycles         cycle   261,096.31
    Total L2 Elapsed Cycles          cycle    8,507,808
    Average SM Active Cycles         cycle   280,421.22
    Total SM Elapsed Cycles          cycle   11,295,450
    Average SMSP Active Cycles       cycle   279,935.49
    Total SMSP Elapsed Cycles        cycle   45,181,800
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      284,900
    Memory Throughput                 %        90.78
    DRAM Throughput                   %        90.78
    Duration                         us       256.80
    L1/TEX Cache Throughput           %        34.44
    L2 Cache Throughput               %        39.16
    SM Active Cycles              cycle   280,614.62
    Compute (SM) Throughput           %        29.02
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.71
    Achieved Active Warps Per SM           warp        37.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,988
    Total DRAM Elapsed Cycles        cycle   14,364,672
    Average L1 Active Cycles         cycle   280,614.62
    Total L1 Elapsed Cycles          cycle   11,289,738
    Average L2 Active Cycles         cycle   260,960.22
    Total L2 Elapsed Cycles          cycle    8,502,624
    Average SM Active Cycles         cycle   280,614.62
    Total SM Elapsed Cycles          cycle   11,289,738
    Average SMSP Active Cycles       cycle   280,176.91
    Total SMSP Elapsed Cycles        cycle   45,158,952
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      285,132
    Memory Throughput                 %        90.71
    DRAM Throughput                   %        90.71
    Duration                         us       256.99
    L1/TEX Cache Throughput           %        34.42
    L2 Cache Throughput               %        39.13
    SM Active Cycles              cycle   280,647.97
    Compute (SM) Throughput           %        29.02
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.73
    Achieved Active Warps Per SM           warp        37.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.27%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,630,134
    Total DRAM Elapsed Cycles        cycle   14,375,936
    Average L1 Active Cycles         cycle   280,647.97
    Total L1 Elapsed Cycles          cycle   11,293,046
    Average L2 Active Cycles         cycle   260,785.19
    Total L2 Elapsed Cycles          cycle    8,509,152
    Average SM Active Cycles         cycle   280,647.97
    Total SM Elapsed Cycles          cycle   11,293,046
    Average SMSP Active Cycles       cycle   280,152.19
    Total SMSP Elapsed Cycles        cycle   45,172,184
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle      285,061
    Memory Throughput                 %        90.66
    DRAM Throughput                   %        90.66
    Duration                         us       257.18
    L1/TEX Cache Throughput           %        34.41
    L2 Cache Throughput               %        39.11
    SM Active Cycles              cycle   280,411.08
    Compute (SM) Throughput           %        28.99
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.77
    Achieved Active Warps Per SM           warp        37.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.23%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,630,238
    Total DRAM Elapsed Cycles        cycle   14,386,176
    Average L1 Active Cycles         cycle   280,411.08
    Total L1 Elapsed Cycles          cycle   11,302,482
    Average L2 Active Cycles         cycle   261,049.19
    Total L2 Elapsed Cycles          cycle    8,512,000
    Average SM Active Cycles         cycle   280,411.08
    Total SM Elapsed Cycles          cycle   11,302,482
    Average SMSP Active Cycles       cycle   280,309.69
    Total SMSP Elapsed Cycles        cycle   45,209,928
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 17, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      284,799
    Memory Throughput                 %        90.78
    DRAM Throughput                   %        90.78
    Duration                         us       256.70
    L1/TEX Cache Throughput           %        34.45
    L2 Cache Throughput               %        39.18
    SM Active Cycles              cycle   280,023.15
    Compute (SM) Throughput           %        29.04
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.10
    Achieved Active Warps Per SM           warp        37.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,226
    Total DRAM Elapsed Cycles        cycle   14,357,504
    Average L1 Active Cycles         cycle   280,023.15
    Total L1 Elapsed Cycles          cycle   11,283,124
    Average L2 Active Cycles         cycle   260,790.03
    Total L2 Elapsed Cycles          cycle    8,499,552
    Average SM Active Cycles         cycle   280,023.15
    Total SM Elapsed Cycles          cycle   11,283,124
    Average SMSP Active Cycles       cycle   279,950.53
    Total SMSP Elapsed Cycles        cycle   45,132,496
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 18, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      284,509
    Memory Throughput                 %        90.88
    DRAM Throughput                   %        90.88
    Duration                         us       256.45
    L1/TEX Cache Throughput           %        34.44
    L2 Cache Throughput               %        39.22
    SM Active Cycles              cycle   280,137.92
    Compute (SM) Throughput           %        29.04
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.66
    Achieved Active Warps Per SM           warp        37.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,452
    Total DRAM Elapsed Cycles        cycle   14,344,192
    Average L1 Active Cycles         cycle   280,137.92
    Total L1 Elapsed Cycles          cycle   11,284,946
    Average L2 Active Cycles         cycle   260,570.25
    Total L2 Elapsed Cycles          cycle    8,490,880
    Average SM Active Cycles         cycle   280,137.92
    Total SM Elapsed Cycles          cycle   11,284,946
    Average SMSP Active Cycles       cycle   279,738.59
    Total SMSP Elapsed Cycles        cycle   45,139,784
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 19, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      284,624
    Memory Throughput                 %        90.85
    DRAM Throughput                   %        90.85
    Duration                         us       256.58
    L1/TEX Cache Throughput           %        34.40
    L2 Cache Throughput               %        39.20
    SM Active Cycles              cycle   279,973.92
    Compute (SM) Throughput           %        28.98
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.90
    Achieved Active Warps Per SM           warp        37.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,804
    Total DRAM Elapsed Cycles        cycle   14,352,384
    Average L1 Active Cycles         cycle   279,973.92
    Total L1 Elapsed Cycles          cycle   11,308,870
    Average L2 Active Cycles         cycle   260,596.97
    Total L2 Elapsed Cycles          cycle    8,494,112
    Average SM Active Cycles         cycle   279,973.92
    Total SM Elapsed Cycles          cycle   11,308,870
    Average SMSP Active Cycles       cycle   279,853.67
    Total SMSP Elapsed Cycles        cycle   45,235,480
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 20, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle      284,497
    Memory Throughput                 %        90.80
    DRAM Throughput                   %        90.80
    Duration                         us       256.74
    L1/TEX Cache Throughput           %        34.48
    L2 Cache Throughput               %        39.18
    SM Active Cycles              cycle   278,748.88
    Compute (SM) Throughput           %        29.05
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.02
    Achieved Active Warps Per SM           warp        37.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,952
    Total DRAM Elapsed Cycles        cycle   14,360,576
    Average L1 Active Cycles         cycle   278,748.88
    Total L1 Elapsed Cycles          cycle   11,281,366
    Average L2 Active Cycles         cycle   261,174.16
    Total L2 Elapsed Cycles          cycle    8,498,048
    Average SM Active Cycles         cycle   278,748.88
    Total SM Elapsed Cycles          cycle   11,281,366
    Average SMSP Active Cycles       cycle   279,792.52
    Total SMSP Elapsed Cycles        cycle   45,125,464
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortHistogramKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, unsigned long long, detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5) (400, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,182,672
    Memory Throughput                 %        88.26
    DRAM Throughput                   %        88.26
    Duration                         ms         1.07
    L1/TEX Cache Throughput           %        76.51
    L2 Cache Throughput               %        41.20
    SM Active Cycles              cycle 1,073,018.62
    Compute (SM) Throughput           %        70.16
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    400
    Registers Per Thread             register/thread              47
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            4.10
    # SMs                                         SM              40
    Threads                                   thread          51,200
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        73.07
    Achieved Active Warps Per SM           warp        35.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.32%                                                                                    
          The difference between calculated theoretical (83.3%) and measured achieved occupancy (73.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    6,577,732
    Total DRAM Elapsed Cycles        cycle   59,623,424
    Average L1 Active Cycles         cycle 1,073,018.62
    Total L1 Elapsed Cycles          cycle   46,805,380
    Average L2 Active Cycles         cycle 1,078,308.50
    Total L2 Elapsed Cycles          cycle   35,289,984
    Average SM Active Cycles         cycle 1,073,018.62
    Total SM Elapsed Cycles          cycle   46,805,380
    Average SMSP Active Cycles       cycle 1,074,361.34
    Total SMSP Elapsed Cycles        cycle  187,221,520
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.609%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 8.30% above the average, while the minimum instance value is 6.06% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.366%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 8.02% above the average, while the minimum instance value is 5.87% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.609%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.30% above the average, while the minimum instance value is 6.06% below the        
          average.                                                                                                      

  void cub::DeviceRadixSortExclusiveSumKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, unsigned long long>(T2 *) (4, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.94
    SM Frequency                    Ghz         1.09
    Elapsed Cycles                cycle        3,703
    Memory Throughput                 %         1.79
    DRAM Throughput                   %         0.93
    Duration                         us         3.39
    L1/TEX Cache Throughput           %         9.02
    L2 Cache Throughput               %         1.79
    SM Active Cycles              cycle       184.03
    Compute (SM) Throughput           %         0.40
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      4
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.34
    # SMs                                         SM              40
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 90%                                                                                             
          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.47
    Achieved Active Warps Per SM           warp         7.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          220
    Total DRAM Elapsed Cycles        cycle      188,416
    Average L1 Active Cycles         cycle       184.03
    Total L1 Elapsed Cycles          cycle      145,104
    Average L2 Active Cycles         cycle       327.16
    Total L2 Elapsed Cycles          cycle      110,464
    Average SM Active Cycles         cycle       184.03
    Total SM Elapsed Cycles          cycle      145,104
    Average SMSP Active Cycles       cycle       183.97
    Total SMSP Elapsed Cycles        cycle      580,416
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.558%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 58.64% above the average, while the minimum instance value is 88.69% below the      
          average.                                                                                                      

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,891,560
    Memory Throughput                 %        86.65
    DRAM Throughput                   %        86.65
    Duration                         ms         4.41
    L1/TEX Cache Throughput           %        63.35
    L2 Cache Throughput               %        45.09
    SM Active Cycles              cycle 4,906,484.45
    Compute (SM) Throughput           %        63.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.37
    Achieved Active Warps Per SM           warp        23.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,706,836
    Total DRAM Elapsed Cycles        cycle  246,571,008
    Average L1 Active Cycles         cycle 4,906,484.45
    Total L1 Elapsed Cycles          cycle  195,468,656
    Average L2 Active Cycles         cycle 4,558,632.38
    Total L2 Elapsed Cycles          cycle  145,955,040
    Average SM Active Cycles         cycle 4,906,484.45
    Total SM Elapsed Cycles          cycle  195,468,656
    Average SMSP Active Cycles       cycle 4,892,982.19
    Total SMSP Elapsed Cycles        cycle  781,874,624
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,541,622
    Memory Throughput                 %        92.84
    DRAM Throughput                   %        92.84
    Duration                         ms         4.09
    L1/TEX Cache Throughput           %        37.35
    L2 Cache Throughput               %        42.22
    SM Active Cycles              cycle 4,519,908.33
    Compute (SM) Throughput           %        31.29
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.42
    Achieved Active Warps Per SM           warp        21.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,567,004
    Total DRAM Elapsed Cycles        cycle  228,931,584
    Average L1 Active Cycles         cycle 4,519,908.33
    Total L1 Elapsed Cycles          cycle  181,682,066
    Average L2 Active Cycles         cycle 4,220,436.94
    Total L2 Elapsed Cycles          cycle  135,513,152
    Average SM Active Cycles         cycle 4,519,908.33
    Total SM Elapsed Cycles          cycle  181,682,066
    Average SMSP Active Cycles       cycle 4,518,285.56
    Total SMSP Elapsed Cycles        cycle  726,728,264
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle    4,903,722
    Memory Throughput                 %        86.45
    DRAM Throughput                   %        86.45
    Duration                         ms         4.42
    L1/TEX Cache Throughput           %        63.64
    L2 Cache Throughput               %        44.97
    SM Active Cycles              cycle 4,885,088.85
    Compute (SM) Throughput           %        63.39
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.37
    Achieved Active Warps Per SM           warp        23.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,714,346
    Total DRAM Elapsed Cycles        cycle  247,214,080
    Average L1 Active Cycles         cycle 4,885,088.85
    Total L1 Elapsed Cycles          cycle  196,079,254
    Average L2 Active Cycles         cycle 4,551,368.69
    Total L2 Elapsed Cycles          cycle  146,332,736
    Average SM Active Cycles         cycle 4,885,088.85
    Total SM Elapsed Cycles          cycle  196,079,254
    Average SMSP Active Cycles       cycle 4,896,984.43
    Total SMSP Elapsed Cycles        cycle  784,317,016
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,537,235
    Memory Throughput                 %        92.91
    DRAM Throughput                   %        92.91
    Duration                         ms         4.09
    L1/TEX Cache Throughput           %        37.33
    L2 Cache Throughput               %        42.33
    SM Active Cycles              cycle 4,525,675.50
    Compute (SM) Throughput           %        31.31
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.50
    Achieved Active Warps Per SM           warp        21.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,562,748
    Total DRAM Elapsed Cycles        cycle  228,708,352
    Average L1 Active Cycles         cycle 4,525,675.50
    Total L1 Elapsed Cycles          cycle  181,651,700
    Average L2 Active Cycles         cycle 4,216,405.59
    Total L2 Elapsed Cycles          cycle  135,381,824
    Average SM Active Cycles         cycle 4,525,675.50
    Total SM Elapsed Cycles          cycle  181,651,700
    Average SMSP Active Cycles       cycle 4,516,935.47
    Total SMSP Elapsed Cycles        cycle  726,606,800
    -------------------------- ----------- ------------

  update_ranks_kernel(const int *, const int *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle   67,565,395
    Memory Throughput                 %        43.69
    DRAM Throughput                   %        28.01
    Duration                         ms        60.87
    L1/TEX Cache Throughput           %        25.31
    L2 Cache Throughput               %        43.69
    SM Active Cycles              cycle   67,554,355
    Compute (SM) Throughput           %         1.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.75
    Achieved Active Warps Per SM           warp        38.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle    119,233,020
    Total DRAM Elapsed Cycles        cycle  3,405,786,112
    Average L1 Active Cycles         cycle     67,554,355
    Total L1 Elapsed Cycles          cycle  2,702,183,706
    Average L2 Active Cycles         cycle  62,967,445.47
    Total L2 Elapsed Cycles          cycle  2,016,006,656
    Average SM Active Cycles         cycle     67,554,355
    Total SM Elapsed Cycles          cycle  2,702,183,706
    Average SMSP Active Cycles       cycle  67,572,245.35
    Total SMSP Elapsed Cycles        cycle 10,808,734,824
    -------------------------- ----------- --------------

  void cub::DeviceScanInitKernel<cub::ScanTileState<int, 1>>(T1, int) (427, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.83
    SM Frequency                    Ghz         1.07
    Elapsed Cycles                cycle        3,305
    Memory Throughput                 %        15.41
    DRAM Throughput                   %         2.72
    Duration                         us         3.07
    L1/TEX Cache Throughput           %        26.81
    L2 Cache Throughput               %        15.41
    SM Active Cycles              cycle     1,281.47
    Compute (SM) Throughput           %         6.47
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.9 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    427
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread          54,656
    Uses Green Context                                             0
    Waves Per SM                                                0.89
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        55.72
    Achieved Active Warps Per SM           warp        26.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (55.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          570
    Total DRAM Elapsed Cycles        cycle      167,936
    Average L1 Active Cycles         cycle     1,281.47
    Total L1 Elapsed Cycles          cycle      132,144
    Average L2 Active Cycles         cycle       866.19
    Total L2 Elapsed Cycles          cycle       98,880
    Average SM Active Cycles         cycle     1,281.47
    Total SM Elapsed Cycles          cycle      132,144
    Average SMSP Active Cycles       cycle     1,225.17
    Total SMSP Elapsed Cycles        cycle      528,576
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.372%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.73% above the average, while the minimum instance value is 15.49% below the      
          average.                                                                                                      

  void cub::DeviceScanKernel<scan::policy_hub<int, int, int, unsigned int, thrust::plus<void>>::Policy1000, thrust::device_ptr<int>, thrust::device_ptr<int>, cub::ScanTileState<int, 1>, thrust::plus<void>, cub::NullType, unsigned int, int, 0>(T2, T3, T4, int, T5, T6, T7) (54614, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle    2,635,544
    Memory Throughput                 %        78.63
    DRAM Throughput                   %        78.63
    Duration                         ms         2.38
    L1/TEX Cache Throughput           %        45.41
    L2 Cache Throughput               %        35.12
    SM Active Cycles              cycle 2,600,088.60
    Compute (SM) Throughput           %        44.55
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 54,614
    Registers Per Thread             register/thread              84
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            7.70
    # SMs                                         SM              40
    Threads                                   thread       6,990,592
    Uses Green Context                                             0
    Waves Per SM                                              273.07
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           20
    Theoretical Occupancy                     %        41.67
    Achieved Occupancy                        %        40.65
    Achieved Active Warps Per SM           warp        19.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 58.33%                                                                                    
          The 5.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (41.7%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   13,113,492
    Total DRAM Elapsed Cycles        cycle  133,425,152
    Average L1 Active Cycles         cycle 2,600,088.60
    Total L1 Elapsed Cycles          cycle  105,982,954
    Average L2 Active Cycles         cycle 2,314,727.34
    Total L2 Elapsed Cycles          cycle   78,899,872
    Average SM Active Cycles         cycle 2,600,088.60
    Total SM Elapsed Cycles          cycle  105,982,954
    Average SMSP Active Cycles       cycle 2,636,157.46
    Total SMSP Elapsed Cycles        cycle  423,931,816
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      285,038
    Memory Throughput                 %        90.73
    DRAM Throughput                   %        90.73
    Duration                         us       256.86
    L1/TEX Cache Throughput           %        34.43
    L2 Cache Throughput               %        39.15
    SM Active Cycles              cycle   280,240.70
    Compute (SM) Throughput           %        29.01
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.83
    Achieved Active Warps Per SM           warp        37.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,750
    Total DRAM Elapsed Cycles        cycle   14,369,792
    Average L1 Active Cycles         cycle   280,240.70
    Total L1 Elapsed Cycles          cycle   11,293,546
    Average L2 Active Cycles         cycle   260,648.75
    Total L2 Elapsed Cycles          cycle    8,505,888
    Average SM Active Cycles         cycle   280,240.70
    Total SM Elapsed Cycles          cycle   11,293,546
    Average SMSP Active Cycles       cycle   279,927.37
    Total SMSP Elapsed Cycles        cycle   45,174,184
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      284,755
    Memory Throughput                 %        90.79
    DRAM Throughput                   %        90.79
    Duration                         us       256.64
    L1/TEX Cache Throughput           %        34.43
    L2 Cache Throughput               %        39.18
    SM Active Cycles              cycle   280,340.75
    Compute (SM) Throughput           %        29.01
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.85
    Achieved Active Warps Per SM           warp        37.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.15%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,628,994
    Total DRAM Elapsed Cycles        cycle   14,354,432
    Average L1 Active Cycles         cycle   280,340.75
    Total L1 Elapsed Cycles          cycle   11,293,730
    Average L2 Active Cycles         cycle   260,578.41
    Total L2 Elapsed Cycles          cycle    8,497,440
    Average SM Active Cycles         cycle   280,340.75
    Total SM Elapsed Cycles          cycle   11,293,730
    Average SMSP Active Cycles       cycle   280,010.31
    Total SMSP Elapsed Cycles        cycle   45,174,920
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      285,002
    Memory Throughput                 %        90.71
    DRAM Throughput                   %        90.71
    Duration                         us       256.86
    L1/TEX Cache Throughput           %        34.38
    L2 Cache Throughput               %        39.15
    SM Active Cycles              cycle   280,460.47
    Compute (SM) Throughput           %        28.99
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.65
    Achieved Active Warps Per SM           warp        37.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,312
    Total DRAM Elapsed Cycles        cycle   14,368,768
    Average L1 Active Cycles         cycle   280,460.47
    Total L1 Elapsed Cycles          cycle   11,301,482
    Average L2 Active Cycles         cycle   260,675.91
    Total L2 Elapsed Cycles          cycle    8,505,312
    Average SM Active Cycles         cycle   280,460.47
    Total SM Elapsed Cycles          cycle   11,301,482
    Average SMSP Active Cycles       cycle   280,200.33
    Total SMSP Elapsed Cycles        cycle   45,205,928
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle      285,171
    Memory Throughput                 %        90.66
    DRAM Throughput                   %        90.66
    Duration                         us       257.18
    L1/TEX Cache Throughput           %        34.40
    L2 Cache Throughput               %        39.11
    SM Active Cycles              cycle   279,290.22
    Compute (SM) Throughput           %        29.01
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.57
    Achieved Active Warps Per SM           warp        37.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,630,448
    Total DRAM Elapsed Cycles        cycle   14,387,200
    Average L1 Active Cycles         cycle   279,290.22
    Total L1 Elapsed Cycles          cycle   11,294,930
    Average L2 Active Cycles         cycle   261,061.69
    Total L2 Elapsed Cycles          cycle    8,513,728
    Average SM Active Cycles         cycle   279,290.22
    Total SM Elapsed Cycles          cycle   11,294,930
    Average SMSP Active Cycles       cycle   280,156.71
    Total SMSP Elapsed Cycles        cycle   45,179,720
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 17, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      284,694
    Memory Throughput                 %        90.82
    DRAM Throughput                   %        90.82
    Duration                         us       256.54
    L1/TEX Cache Throughput           %        34.45
    L2 Cache Throughput               %        39.20
    SM Active Cycles              cycle   280,156.17
    Compute (SM) Throughput           %        29.03
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.06
    Achieved Active Warps Per SM           warp        37.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,118
    Total DRAM Elapsed Cycles        cycle   14,350,336
    Average L1 Active Cycles         cycle   280,156.17
    Total L1 Elapsed Cycles          cycle   11,287,104
    Average L2 Active Cycles         cycle   260,518.78
    Total L2 Elapsed Cycles          cycle    8,495,520
    Average SM Active Cycles         cycle   280,156.17
    Total SM Elapsed Cycles          cycle   11,287,104
    Average SMSP Active Cycles       cycle   280,053.76
    Total SMSP Elapsed Cycles        cycle   45,148,416
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 18, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      284,673
    Memory Throughput                 %        90.81
    DRAM Throughput                   %        90.81
    Duration                         us       256.58
    L1/TEX Cache Throughput           %        34.62
    L2 Cache Throughput               %        39.19
    SM Active Cycles              cycle   278,888.22
    Compute (SM) Throughput           %        29.16
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.81
    Achieved Active Warps Per SM           warp        37.35
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,222
    Total DRAM Elapsed Cycles        cycle   14,353,408
    Average L1 Active Cycles         cycle   278,888.22
    Total L1 Elapsed Cycles          cycle   11,238,314
    Average L2 Active Cycles         cycle   260,942.78
    Total L2 Elapsed Cycles          cycle    8,495,744
    Average SM Active Cycles         cycle   278,888.22
    Total SM Elapsed Cycles          cycle   11,238,314
    Average SMSP Active Cycles       cycle   278,526.95
    Total SMSP Elapsed Cycles        cycle   44,953,256
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 19, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      285,267
    Memory Throughput                 %        90.84
    DRAM Throughput                   %        90.84
    Duration                         us       257.09
    L1/TEX Cache Throughput           %        34.46
    L2 Cache Throughput               %        39.11
    SM Active Cycles              cycle   279,994.10
    Compute (SM) Throughput           %        29.03
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.85
    Achieved Active Warps Per SM           warp        37.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.15%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,632,976
    Total DRAM Elapsed Cycles        cycle   14,381,056
    Average L1 Active Cycles         cycle   279,994.10
    Total L1 Elapsed Cycles          cycle   11,286,430
    Average L2 Active Cycles         cycle   260,749.59
    Total L2 Elapsed Cycles          cycle    8,512,704
    Average SM Active Cycles         cycle   279,994.10
    Total SM Elapsed Cycles          cycle   11,286,430
    Average SMSP Active Cycles       cycle   279,740.31
    Total SMSP Elapsed Cycles        cycle   45,145,720
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 20, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle      284,691
    Memory Throughput                 %        90.76
    DRAM Throughput                   %        90.76
    Duration                         us       256.77
    L1/TEX Cache Throughput           %        34.48
    L2 Cache Throughput               %        39.18
    SM Active Cycles              cycle   278,562.70
    Compute (SM) Throughput           %        29.05
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.96
    Achieved Active Warps Per SM           warp        37.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,700
    Total DRAM Elapsed Cycles        cycle   14,364,672
    Average L1 Active Cycles         cycle   278,562.70
    Total L1 Elapsed Cycles          cycle   11,279,474
    Average L2 Active Cycles         cycle   260,668.88
    Total L2 Elapsed Cycles          cycle    8,500,064
    Average SM Active Cycles         cycle   278,562.70
    Total SM Elapsed Cycles          cycle   11,279,474
    Average SMSP Active Cycles       cycle   280,129.59
    Total SMSP Elapsed Cycles        cycle   45,117,896
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortHistogramKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, unsigned long long, detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5) (400, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,160,614
    Memory Throughput                 %        90.01
    DRAM Throughput                   %        90.01
    Duration                         ms         1.05
    L1/TEX Cache Throughput           %        76.95
    L2 Cache Throughput               %        42.79
    SM Active Cycles              cycle 1,066,789.98
    Compute (SM) Throughput           %        71.07
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    400
    Registers Per Thread             register/thread              47
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            4.10
    # SMs                                         SM              40
    Threads                                   thread          51,200
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        74.51
    Achieved Active Warps Per SM           warp        35.77
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    6,583,030
    Total DRAM Elapsed Cycles        cycle   58,511,360
    Average L1 Active Cycles         cycle 1,066,789.98
    Total L1 Elapsed Cycles          cycle   46,199,976
    Average L2 Active Cycles         cycle 1,066,109.38
    Total L2 Elapsed Cycles          cycle   34,632,160
    Average SM Active Cycles         cycle 1,066,789.98
    Total SM Elapsed Cycles          cycle   46,199,976
    Average SMSP Active Cycles       cycle 1,066,772.72
    Total SMSP Elapsed Cycles        cycle  184,799,904
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.184%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 7.78% above the average, while the minimum instance value is 5.34% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.152%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 7.74% above the average, while the minimum instance value is 5.51% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.184%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 7.78% above the average, while the minimum instance value is 5.34% below the        
          average.                                                                                                      

  void cub::DeviceRadixSortExclusiveSumKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, unsigned long long>(T2 *) (4, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.87
    SM Frequency                    Ghz         1.08
    Elapsed Cycles                cycle        3,685
    Memory Throughput                 %         1.80
    DRAM Throughput                   %         0.96
    Duration                         us         3.39
    L1/TEX Cache Throughput           %         8.70
    L2 Cache Throughput               %         1.80
    SM Active Cycles              cycle       190.90
    Compute (SM) Throughput           %         0.40
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      4
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.34
    # SMs                                         SM              40
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 90%                                                                                             
          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.48
    Achieved Active Warps Per SM           warp         7.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          224
    Total DRAM Elapsed Cycles        cycle      186,368
    Average L1 Active Cycles         cycle       190.90
    Total L1 Elapsed Cycles          cycle      144,584
    Average L2 Active Cycles         cycle       335.97
    Total L2 Elapsed Cycles          cycle      109,984
    Average SM Active Cycles         cycle       190.90
    Total SM Elapsed Cycles          cycle      144,584
    Average SMSP Active Cycles       cycle       185.38
    Total SMSP Elapsed Cycles        cycle      578,336
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.239%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 53.60% above the average, while the minimum instance value is 88.99% below the      
          average.                                                                                                      

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,543,425
    Memory Throughput                 %        92.76
    DRAM Throughput                   %        92.76
    Duration                         ms         4.09
    L1/TEX Cache Throughput           %        41.98
    L2 Cache Throughput               %        43.49
    SM Active Cycles              cycle 4,532,809.55
    Compute (SM) Throughput           %        31.26
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.52
    Achieved Active Warps Per SM           warp        21.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,555,690
    Total DRAM Elapsed Cycles        cycle  229,023,744
    Average L1 Active Cycles         cycle 4,532,809.55
    Total L1 Elapsed Cycles          cycle  181,725,656
    Average L2 Active Cycles         cycle 4,218,259.19
    Total L2 Elapsed Cycles          cycle  135,567,168
    Average SM Active Cycles         cycle 4,532,809.55
    Total SM Elapsed Cycles          cycle  181,725,656
    Average SMSP Active Cycles       cycle 4,532,070.92
    Total SMSP Elapsed Cycles        cycle  726,902,624
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,532,620
    Memory Throughput                 %        92.99
    DRAM Throughput                   %        92.99
    Duration                         ms         4.08
    L1/TEX Cache Throughput           %        41.57
    L2 Cache Throughput               %        43.55
    SM Active Cycles              cycle 4,527,888.65
    Compute (SM) Throughput           %        31.47
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.54
    Achieved Active Warps Per SM           warp        21.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,558,368
    Total DRAM Elapsed Cycles        cycle  228,476,928
    Average L1 Active Cycles         cycle 4,527,888.65
    Total L1 Elapsed Cycles          cycle  181,336,930
    Average L2 Active Cycles         cycle 4,230,777.75
    Total L2 Elapsed Cycles          cycle  135,243,936
    Average SM Active Cycles         cycle 4,527,888.65
    Total SM Elapsed Cycles          cycle  181,336,930
    Average SMSP Active Cycles       cycle 4,523,396.12
    Total SMSP Elapsed Cycles        cycle  725,347,720
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,542,893
    Memory Throughput                 %        92.78
    DRAM Throughput                   %        92.78
    Duration                         ms         4.09
    L1/TEX Cache Throughput           %        42.05
    L2 Cache Throughput               %        43.57
    SM Active Cycles              cycle 4,529,843.35
    Compute (SM) Throughput           %        31.52
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.59
    Achieved Active Warps Per SM           warp        21.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,557,634
    Total DRAM Elapsed Cycles        cycle  228,995,072
    Average L1 Active Cycles         cycle 4,529,843.35
    Total L1 Elapsed Cycles          cycle  181,439,820
    Average L2 Active Cycles         cycle 4,225,431.66
    Total L2 Elapsed Cycles          cycle  135,551,040
    Average SM Active Cycles         cycle 4,529,843.35
    Total SM Elapsed Cycles          cycle  181,439,820
    Average SMSP Active Cycles       cycle 4,519,753.38
    Total SMSP Elapsed Cycles        cycle  725,759,280
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,540,225
    Memory Throughput                 %        92.85
    DRAM Throughput                   %        92.85
    Duration                         ms         4.09
    L1/TEX Cache Throughput           %        41.07
    L2 Cache Throughput               %        43.39
    SM Active Cycles              cycle 4,524,997.97
    Compute (SM) Throughput           %        31.56
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.72
    Achieved Active Warps Per SM           warp        21.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,562,982
    Total DRAM Elapsed Cycles        cycle  228,857,856
    Average L1 Active Cycles         cycle 4,524,997.97
    Total L1 Elapsed Cycles          cycle  181,101,308
    Average L2 Active Cycles         cycle 4,213,940.19
    Total L2 Elapsed Cycles          cycle  135,470,528
    Average SM Active Cycles         cycle 4,524,997.97
    Total SM Elapsed Cycles          cycle  181,101,308
    Average SMSP Active Cycles       cycle 4,519,927.74
    Total SMSP Elapsed Cycles        cycle  724,405,232
    -------------------------- ----------- ------------

  update_ranks_kernel(const int *, const int *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle    67,563,486
    Memory Throughput                 %         43.65
    DRAM Throughput                   %         28.01
    Duration                         ms         60.87
    L1/TEX Cache Throughput           %         25.32
    L2 Cache Throughput               %         43.65
    SM Active Cycles              cycle 67,610,827.03
    Compute (SM) Throughput           %          1.94
    ----------------------- ----------- -------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.72
    Achieved Active Warps Per SM           warp        38.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle    119,238,184
    Total DRAM Elapsed Cycles        cycle  3,405,690,880
    Average L1 Active Cycles         cycle  67,610,827.03
    Total L1 Elapsed Cycles          cycle  2,702,130,776
    Average L2 Active Cycles         cycle  63,015,489.03
    Total L2 Elapsed Cycles          cycle  2,015,949,536
    Average SM Active Cycles         cycle  67,610,827.03
    Total SM Elapsed Cycles          cycle  2,702,130,776
    Average SMSP Active Cycles       cycle  67,581,578.19
    Total SMSP Elapsed Cycles        cycle 10,808,523,104
    -------------------------- ----------- --------------

  void cub::DeviceScanInitKernel<cub::ScanTileState<int, 1>>(T1, int) (427, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.85
    SM Frequency                    Ghz         1.07
    Elapsed Cycles                cycle        3,329
    Memory Throughput                 %        15.16
    DRAM Throughput                   %         3.58
    Duration                         us         3.10
    L1/TEX Cache Throughput           %        26.77
    L2 Cache Throughput               %        15.16
    SM Active Cycles              cycle     1,283.20
    Compute (SM) Throughput           %         6.55
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.9 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    427
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread          54,656
    Uses Green Context                                             0
    Waves Per SM                                                0.89
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        55.62
    Achieved Active Warps Per SM           warp        26.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (55.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          760
    Total DRAM Elapsed Cycles        cycle      169,984
    Average L1 Active Cycles         cycle     1,283.20
    Total L1 Elapsed Cycles          cycle      130,548
    Average L2 Active Cycles         cycle       886.62
    Total L2 Elapsed Cycles          cycle       99,712
    Average SM Active Cycles         cycle     1,283.20
    Total SM Elapsed Cycles          cycle      130,548
    Average SMSP Active Cycles       cycle     1,318.83
    Total SMSP Elapsed Cycles        cycle      522,192
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.76%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 30.79% above the average, while the minimum instance value is 14.28% below the      
          average.                                                                                                      

  void cub::DeviceScanKernel<scan::policy_hub<int, int, int, unsigned int, thrust::plus<void>>::Policy1000, thrust::device_ptr<int>, thrust::device_ptr<int>, cub::ScanTileState<int, 1>, thrust::plus<void>, cub::NullType, unsigned int, int, 0>(T2, T3, T4, int, T5, T6, T7) (54614, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    2,651,560
    Memory Throughput                 %        78.50
    DRAM Throughput                   %        78.50
    Duration                         ms         2.39
    L1/TEX Cache Throughput           %        44.56
    L2 Cache Throughput               %        35.01
    SM Active Cycles              cycle 2,648,990.98
    Compute (SM) Throughput           %        44.80
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 54,614
    Registers Per Thread             register/thread              84
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            7.70
    # SMs                                         SM              40
    Threads                                   thread       6,990,592
    Uses Green Context                                             0
    Waves Per SM                                              273.07
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           20
    Theoretical Occupancy                     %        41.67
    Achieved Occupancy                        %        40.63
    Achieved Active Warps Per SM           warp        19.50
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 58.33%                                                                                    
          The 5.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (41.7%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   13,116,296
    Total DRAM Elapsed Cycles        cycle  133,677,056
    Average L1 Active Cycles         cycle 2,648,990.98
    Total L1 Elapsed Cycles          cycle  105,407,774
    Average L2 Active Cycles         cycle 2,326,576.88
    Total L2 Elapsed Cycles          cycle   79,128,128
    Average SM Active Cycles         cycle 2,648,990.98
    Total SM Elapsed Cycles          cycle  105,407,774
    Average SMSP Active Cycles       cycle 2,635,628.47
    Total SMSP Elapsed Cycles        cycle  421,631,096
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle      285,131
    Memory Throughput                 %        90.66
    DRAM Throughput                   %        90.66
    Duration                         us       257.18
    L1/TEX Cache Throughput           %        34.39
    L2 Cache Throughput               %        39.13
    SM Active Cycles              cycle   280,430.85
    Compute (SM) Throughput           %        28.97
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.88
    Achieved Active Warps Per SM           warp        37.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.12%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,630,338
    Total DRAM Elapsed Cycles        cycle   14,386,176
    Average L1 Active Cycles         cycle   280,430.85
    Total L1 Elapsed Cycles          cycle   11,311,242
    Average L2 Active Cycles         cycle   261,004.06
    Total L2 Elapsed Cycles          cycle    8,512,448
    Average SM Active Cycles         cycle   280,430.85
    Total SM Elapsed Cycles          cycle   11,311,242
    Average SMSP Active Cycles       cycle   280,257.64
    Total SMSP Elapsed Cycles        cycle   45,244,968
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle      284,725
    Memory Throughput                 %        90.73
    DRAM Throughput                   %        90.73
    Duration                         us       256.83
    L1/TEX Cache Throughput           %        34.42
    L2 Cache Throughput               %        39.17
    SM Active Cycles              cycle   280,165.83
    Compute (SM) Throughput           %        29.01
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.67
    Achieved Active Warps Per SM           warp        37.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,170
    Total DRAM Elapsed Cycles        cycle   14,364,672
    Average L1 Active Cycles         cycle   280,165.83
    Total L1 Elapsed Cycles          cycle   11,294,430
    Average L2 Active Cycles         cycle   260,965.16
    Total L2 Elapsed Cycles          cycle    8,500,448
    Average SM Active Cycles         cycle   280,165.83
    Total SM Elapsed Cycles          cycle   11,294,430
    Average SMSP Active Cycles       cycle   280,162.93
    Total SMSP Elapsed Cycles        cycle   45,177,720
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      285,086
    Memory Throughput                 %        90.72
    DRAM Throughput                   %        90.72
    Duration                         us       256.99
    L1/TEX Cache Throughput           %        34.41
    L2 Cache Throughput               %        39.13
    SM Active Cycles              cycle   280,525.03
    Compute (SM) Throughput           %        29.00
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.75
    Achieved Active Warps Per SM           warp        37.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,630,066
    Total DRAM Elapsed Cycles        cycle   14,374,912
    Average L1 Active Cycles         cycle   280,525.03
    Total L1 Elapsed Cycles          cycle   11,298,696
    Average L2 Active Cycles         cycle   260,948.16
    Total L2 Elapsed Cycles          cycle    8,508,160
    Average SM Active Cycles         cycle   280,525.03
    Total SM Elapsed Cycles          cycle   11,298,696
    Average SMSP Active Cycles       cycle   280,340.30
    Total SMSP Elapsed Cycles        cycle   45,194,784
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      285,304
    Memory Throughput                 %        90.70
    DRAM Throughput                   %        90.70
    Duration                         us       257.12
    L1/TEX Cache Throughput           %        34.41
    L2 Cache Throughput               %        39.11
    SM Active Cycles              cycle   280,312.08
    Compute (SM) Throughput           %        29.00
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.67
    Achieved Active Warps Per SM           warp        37.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,630,510
    Total DRAM Elapsed Cycles        cycle   14,382,080
    Average L1 Active Cycles         cycle   280,312.08
    Total L1 Elapsed Cycles          cycle   11,300,662
    Average L2 Active Cycles         cycle   261,036.94
    Total L2 Elapsed Cycles          cycle    8,513,760
    Average SM Active Cycles         cycle   280,312.08
    Total SM Elapsed Cycles          cycle   11,300,662
    Average SMSP Active Cycles       cycle   280,258.94
    Total SMSP Elapsed Cycles        cycle   45,202,648
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 17, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      284,880
    Memory Throughput                 %        90.79
    DRAM Throughput                   %        90.79
    Duration                         us       256.77
    L1/TEX Cache Throughput           %        34.44
    L2 Cache Throughput               %        39.16
    SM Active Cycles              cycle   280,155.05
    Compute (SM) Throughput           %        29.03
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.07
    Achieved Active Warps Per SM           warp        37.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,630,134
    Total DRAM Elapsed Cycles        cycle   14,363,648
    Average L1 Active Cycles         cycle   280,155.05
    Total L1 Elapsed Cycles          cycle   11,287,678
    Average L2 Active Cycles         cycle   260,553.94
    Total L2 Elapsed Cycles          cycle    8,501,888
    Average SM Active Cycles         cycle   280,155.05
    Total SM Elapsed Cycles          cycle   11,287,678
    Average SMSP Active Cycles       cycle   279,808.80
    Total SMSP Elapsed Cycles        cycle   45,150,712
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 18, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      284,580
    Memory Throughput                 %        90.84
    DRAM Throughput                   %        90.84
    Duration                         us       256.54
    L1/TEX Cache Throughput           %        34.41
    L2 Cache Throughput               %        39.20
    SM Active Cycles              cycle   280,128.95
    Compute (SM) Throughput           %        29.02
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.71
    Achieved Active Warps Per SM           warp        37.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,552
    Total DRAM Elapsed Cycles        cycle   14,350,336
    Average L1 Active Cycles         cycle   280,128.95
    Total L1 Elapsed Cycles          cycle   11,290,144
    Average L2 Active Cycles         cycle   260,434.72
    Total L2 Elapsed Cycles          cycle    8,493,184
    Average SM Active Cycles         cycle   280,128.95
    Total SM Elapsed Cycles          cycle   11,290,144
    Average SMSP Active Cycles       cycle   279,790.75
    Total SMSP Elapsed Cycles        cycle   45,160,576
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 19, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.07
    Elapsed Cycles                cycle      279,649
    Memory Throughput                 %        90.67
    DRAM Throughput                   %        90.67
    Duration                         us       257.12
    L1/TEX Cache Throughput           %        34.47
    L2 Cache Throughput               %        39.31
    SM Active Cycles              cycle   279,978.33
    Compute (SM) Throughput           %        29.05
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.98
    Achieved Active Warps Per SM           warp        37.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,630,158
    Total DRAM Elapsed Cycles        cycle   14,383,104
    Average L1 Active Cycles         cycle   279,978.33
    Total L1 Elapsed Cycles          cycle   11,281,714
    Average L2 Active Cycles         cycle   260,580.78
    Total L2 Elapsed Cycles          cycle    8,470,144
    Average SM Active Cycles         cycle   279,978.33
    Total SM Elapsed Cycles          cycle   11,281,714
    Average SMSP Active Cycles       cycle   279,946.36
    Total SMSP Elapsed Cycles        cycle   45,126,856
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 20, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      284,623
    Memory Throughput                 %        90.87
    DRAM Throughput                   %        90.87
    Duration                         us       256.54
    L1/TEX Cache Throughput           %        34.49
    L2 Cache Throughput               %        39.20
    SM Active Cycles              cycle   280,059.40
    Compute (SM) Throughput           %        29.06
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.19
    Achieved Active Warps Per SM           warp        37.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.81%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,932
    Total DRAM Elapsed Cycles        cycle   14,350,336
    Average L1 Active Cycles         cycle   280,059.40
    Total L1 Elapsed Cycles          cycle   11,275,972
    Average L2 Active Cycles         cycle   260,532.25
    Total L2 Elapsed Cycles          cycle    8,494,336
    Average SM Active Cycles         cycle   280,059.40
    Total SM Elapsed Cycles          cycle   11,275,972
    Average SMSP Active Cycles       cycle   279,669.59
    Total SMSP Elapsed Cycles        cycle   45,103,888
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortHistogramKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, unsigned long long, detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5) (400, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,160,358
    Memory Throughput                 %        89.96
    DRAM Throughput                   %        89.96
    Duration                         ms         1.05
    L1/TEX Cache Throughput           %        76.96
    L2 Cache Throughput               %        43.03
    SM Active Cycles              cycle 1,066,818.93
    Compute (SM) Throughput           %        71.04
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    400
    Registers Per Thread             register/thread              47
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            4.10
    # SMs                                         SM              40
    Threads                                   thread          51,200
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        74.38
    Achieved Active Warps Per SM           warp        35.70
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    6,578,584
    Total DRAM Elapsed Cycles        cycle   58,499,072
    Average L1 Active Cycles         cycle 1,066,818.93
    Total L1 Elapsed Cycles          cycle   46,231,146
    Average L2 Active Cycles         cycle 1,066,603.41
    Total L2 Elapsed Cycles          cycle   34,624,736
    Average SM Active Cycles         cycle 1,066,818.93
    Total SM Elapsed Cycles          cycle   46,231,146
    Average SMSP Active Cycles       cycle 1,066,864.36
    Total SMSP Elapsed Cycles        cycle  184,924,584
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.35%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 7.96% above the average, while the minimum instance value is 5.66% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.087%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 7.68% above the average, while the minimum instance value is 5.35% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.35%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 7.96% above the average, while the minimum instance value is 5.66% below the        
          average.                                                                                                      

  void cub::DeviceRadixSortExclusiveSumKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, unsigned long long>(T2 *) (4, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.86
    SM Frequency                    Ghz         1.09
    Elapsed Cycles                cycle        3,652
    Memory Throughput                 %         1.81
    DRAM Throughput                   %         0.99
    Duration                         us         3.36
    L1/TEX Cache Throughput           %         9.01
    L2 Cache Throughput               %         1.81
    SM Active Cycles              cycle       184.28
    Compute (SM) Throughput           %         0.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      4
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.34
    # SMs                                         SM              40
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 90%                                                                                             
          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.48
    Achieved Active Warps Per SM           warp         7.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          228
    Total DRAM Elapsed Cycles        cycle      184,320
    Average L1 Active Cycles         cycle       184.28
    Total L1 Elapsed Cycles          cycle      143,730
    Average L2 Active Cycles         cycle       334.34
    Total L2 Elapsed Cycles          cycle      108,928
    Average SM Active Cycles         cycle       184.28
    Total SM Elapsed Cycles          cycle      143,730
    Average SMSP Active Cycles       cycle       184.57
    Total SMSP Elapsed Cycles        cycle      574,920
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.157%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 52.51% above the average, while the minimum instance value is 88.93% below the      
          average.                                                                                                      

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,560,705
    Memory Throughput                 %        92.44
    DRAM Throughput                   %        92.44
    Duration                         ms         4.11
    L1/TEX Cache Throughput           %        42.26
    L2 Cache Throughput               %        43.40
    SM Active Cycles              cycle 4,535,833.40
    Compute (SM) Throughput           %        33.27
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        46.18
    Achieved Active Warps Per SM           warp        22.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,564,222
    Total DRAM Elapsed Cycles        cycle  229,896,192
    Average L1 Active Cycles         cycle 4,535,833.40
    Total L1 Elapsed Cycles          cycle  181,861,124
    Average L2 Active Cycles         cycle 4,233,357.38
    Total L2 Elapsed Cycles          cycle  136,083,488
    Average SM Active Cycles         cycle 4,535,833.40
    Total SM Elapsed Cycles          cycle  181,861,124
    Average SMSP Active Cycles       cycle 4,541,070.15
    Total SMSP Elapsed Cycles        cycle  727,444,496
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,558,691
    Memory Throughput                 %        92.51
    DRAM Throughput                   %        92.51
    Duration                         ms         4.11
    L1/TEX Cache Throughput           %        42.91
    L2 Cache Throughput               %        43.60
    SM Active Cycles              cycle 4,535,081.58
    Compute (SM) Throughput           %        33.46
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        46.07
    Achieved Active Warps Per SM           warp        22.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,572,942
    Total DRAM Elapsed Cycles        cycle  229,788,672
    Average L1 Active Cycles         cycle 4,535,081.58
    Total L1 Elapsed Cycles          cycle  181,882,514
    Average L2 Active Cycles         cycle    4,230,629
    Total L2 Elapsed Cycles          cycle  136,021,280
    Average SM Active Cycles         cycle 4,535,081.58
    Total SM Elapsed Cycles          cycle  181,882,514
    Average SMSP Active Cycles       cycle 4,544,907.56
    Total SMSP Elapsed Cycles        cycle  727,530,056
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,556,715
    Memory Throughput                 %        92.51
    DRAM Throughput                   %        92.51
    Duration                         ms         4.11
    L1/TEX Cache Throughput           %        42.23
    L2 Cache Throughput               %        43.42
    SM Active Cycles              cycle    4,542,323
    Compute (SM) Throughput           %        33.00
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.70
    Achieved Active Warps Per SM           warp        21.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,560,644
    Total DRAM Elapsed Cycles        cycle  229,693,440
    Average L1 Active Cycles         cycle    4,542,323
    Total L1 Elapsed Cycles          cycle  182,283,474
    Average L2 Active Cycles         cycle 4,235,328.50
    Total L2 Elapsed Cycles          cycle  135,963,520
    Average SM Active Cycles         cycle    4,542,323
    Total SM Elapsed Cycles          cycle  182,283,474
    Average SMSP Active Cycles       cycle 4,538,113.12
    Total SMSP Elapsed Cycles        cycle  729,133,896
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle    4,535,993
    Memory Throughput                 %        92.89
    DRAM Throughput                   %        92.89
    Duration                         ms         4.09
    L1/TEX Cache Throughput           %        42.22
    L2 Cache Throughput               %        43.59
    SM Active Cycles              cycle 4,502,219.10
    Compute (SM) Throughput           %        33.03
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.88
    Achieved Active Warps Per SM           warp        22.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,565,838
    Total DRAM Elapsed Cycles        cycle  228,788,224
    Average L1 Active Cycles         cycle 4,502,219.10
    Total L1 Elapsed Cycles          cycle  181,484,936
    Average L2 Active Cycles         cycle 4,220,890.78
    Total L2 Elapsed Cycles          cycle  135,423,552
    Average SM Active Cycles         cycle 4,502,219.10
    Total SM Elapsed Cycles          cycle  181,484,936
    Average SMSP Active Cycles       cycle 4,532,135.83
    Total SMSP Elapsed Cycles        cycle  725,939,744
    -------------------------- ----------- ------------

  update_ranks_kernel(const int *, const int *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle    67,561,192
    Memory Throughput                 %         43.65
    DRAM Throughput                   %         28.01
    Duration                         ms         60.87
    L1/TEX Cache Throughput           %         25.30
    L2 Cache Throughput               %         43.65
    SM Active Cycles              cycle 67,553,825.65
    Compute (SM) Throughput           %          1.94
    ----------------------- ----------- -------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.72
    Achieved Active Warps Per SM           warp        38.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle    119,239,022
    Total DRAM Elapsed Cycles        cycle  3,405,574,144
    Average L1 Active Cycles         cycle  67,553,825.65
    Total L1 Elapsed Cycles          cycle  2,703,833,334
    Average L2 Active Cycles         cycle  63,003,942.47
    Total L2 Elapsed Cycles          cycle  2,015,880,800
    Average SM Active Cycles         cycle  67,553,825.65
    Total SM Elapsed Cycles          cycle  2,703,833,334
    Average SMSP Active Cycles       cycle  67,568,906.84
    Total SMSP Elapsed Cycles        cycle 10,815,333,336
    -------------------------- ----------- --------------

  void cub::DeviceScanInitKernel<cub::ScanTileState<int, 1>>(T1, int) (427, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.85
    SM Frequency                    Ghz         1.07
    Elapsed Cycles                cycle        3,333
    Memory Throughput                 %        15.19
    DRAM Throughput                   %         4.74
    Duration                         us         3.10
    L1/TEX Cache Throughput           %        26.80
    L2 Cache Throughput               %        15.19
    SM Active Cycles              cycle     1,282.17
    Compute (SM) Throughput           %         6.49
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.9 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    427
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread          54,656
    Uses Green Context                                             0
    Waves Per SM                                                0.89
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        55.75
    Achieved Active Warps Per SM           warp        26.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (55.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        1,008
    Total DRAM Elapsed Cycles        cycle      169,984
    Average L1 Active Cycles         cycle     1,282.17
    Total L1 Elapsed Cycles          cycle      131,792
    Average L2 Active Cycles         cycle       864.22
    Total L2 Elapsed Cycles          cycle       99,584
    Average SM Active Cycles         cycle     1,282.17
    Total SM Elapsed Cycles          cycle      131,792
    Average SMSP Active Cycles       cycle     1,224.29
    Total SMSP Elapsed Cycles        cycle      527,168
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.099%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 25.56% above the average, while the minimum instance value is 14.72% below the      
          average.                                                                                                      

  void cub::DeviceScanKernel<scan::policy_hub<int, int, int, unsigned int, thrust::plus<void>>::Policy1000, thrust::device_ptr<int>, thrust::device_ptr<int>, cub::ScanTileState<int, 1>, thrust::plus<void>, cub::NullType, unsigned int, int, 0>(T2, T3, T4, int, T5, T6, T7) (54614, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    2,644,000
    Memory Throughput                 %        78.68
    DRAM Throughput                   %        78.68
    Duration                         ms         2.38
    L1/TEX Cache Throughput           %        44.96
    L2 Cache Throughput               %        35.11
    SM Active Cycles              cycle 2,625,715.95
    Compute (SM) Throughput           %        44.62
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 54,614
    Registers Per Thread             register/thread              84
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            7.70
    # SMs                                         SM              40
    Threads                                   thread       6,990,592
    Uses Green Context                                             0
    Waves Per SM                                              273.07
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           20
    Theoretical Occupancy                     %        41.67
    Achieved Occupancy                        %        40.64
    Achieved Active Warps Per SM           warp        19.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 58.33%                                                                                    
          The 5.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (41.7%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   13,112,366
    Total DRAM Elapsed Cycles        cycle  133,325,824
    Average L1 Active Cycles         cycle 2,625,715.95
    Total L1 Elapsed Cycles          cycle  105,825,158
    Average L2 Active Cycles         cycle 2,316,447.88
    Total L2 Elapsed Cycles          cycle   78,919,040
    Average SM Active Cycles         cycle 2,625,715.95
    Total SM Elapsed Cycles          cycle  105,825,158
    Average SMSP Active Cycles       cycle 2,624,662.85
    Total SMSP Elapsed Cycles        cycle  423,300,632
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle      284,779
    Memory Throughput                 %        90.74
    DRAM Throughput                   %        90.74
    Duration                         us       256.83
    L1/TEX Cache Throughput           %        34.61
    L2 Cache Throughput               %        39.18
    SM Active Cycles              cycle   278,621.22
    Compute (SM) Throughput           %        29.16
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.84
    Achieved Active Warps Per SM           warp        37.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.16%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,616
    Total DRAM Elapsed Cycles        cycle   14,367,744
    Average L1 Active Cycles         cycle   278,621.22
    Total L1 Elapsed Cycles          cycle   11,236,872
    Average L2 Active Cycles         cycle   260,812.66
    Total L2 Elapsed Cycles          cycle    8,502,048
    Average SM Active Cycles         cycle   278,621.22
    Total SM Elapsed Cycles          cycle   11,236,872
    Average SMSP Active Cycles       cycle   278,625.23
    Total SMSP Elapsed Cycles        cycle   44,947,488
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      284,812
    Memory Throughput                 %        90.86
    DRAM Throughput                   %        90.86
    Duration                         us       256.74
    L1/TEX Cache Throughput           %        34.38
    L2 Cache Throughput               %        39.17
    SM Active Cycles              cycle   279,891.60
    Compute (SM) Throughput           %        28.98
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.85
    Achieved Active Warps Per SM           warp        37.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.15%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,631,108
    Total DRAM Elapsed Cycles        cycle   14,361,600
    Average L1 Active Cycles         cycle   279,891.60
    Total L1 Elapsed Cycles          cycle   11,305,356
    Average L2 Active Cycles         cycle   260,792.59
    Total L2 Elapsed Cycles          cycle    8,499,872
    Average SM Active Cycles         cycle   279,891.60
    Total SM Elapsed Cycles          cycle   11,305,356
    Average SMSP Active Cycles       cycle   280,214.16
    Total SMSP Elapsed Cycles        cycle   45,221,424
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      285,486
    Memory Throughput                 %        90.57
    DRAM Throughput                   %        90.57
    Duration                         us       257.31
    L1/TEX Cache Throughput           %        34.38
    L2 Cache Throughput               %        39.10
    SM Active Cycles              cycle   280,237.45
    Compute (SM) Throughput           %        28.98
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.57
    Achieved Active Warps Per SM           warp        37.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,708
    Total DRAM Elapsed Cycles        cycle   14,395,392
    Average L1 Active Cycles         cycle   280,237.45
    Total L1 Elapsed Cycles          cycle   11,305,450
    Average L2 Active Cycles         cycle   260,734.41
    Total L2 Elapsed Cycles          cycle    8,519,936
    Average SM Active Cycles         cycle   280,237.45
    Total SM Elapsed Cycles          cycle   11,305,450
    Average SMSP Active Cycles       cycle   280,154.62
    Total SMSP Elapsed Cycles        cycle   45,221,800
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      285,231
    Memory Throughput                 %        90.66
    DRAM Throughput                   %        90.66
    Duration                         us       257.09
    L1/TEX Cache Throughput           %        34.40
    L2 Cache Throughput               %        39.43
    SM Active Cycles              cycle   280,449.30
    Compute (SM) Throughput           %        29.02
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.52
    Achieved Active Warps Per SM           warp        37.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.48%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,788
    Total DRAM Elapsed Cycles        cycle   14,381,056
    Average L1 Active Cycles         cycle   280,449.30
    Total L1 Elapsed Cycles          cycle   11,292,132
    Average L2 Active Cycles         cycle   260,925.78
    Total L2 Elapsed Cycles          cycle    8,512,352
    Average SM Active Cycles         cycle   280,449.30
    Total SM Elapsed Cycles          cycle   11,292,132
    Average SMSP Active Cycles       cycle   280,222.24
    Total SMSP Elapsed Cycles        cycle   45,168,528
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 17, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      284,541
    Memory Throughput                 %        90.87
    DRAM Throughput                   %        90.87
    Duration                         us       256.48
    L1/TEX Cache Throughput           %        34.46
    L2 Cache Throughput               %        39.21
    SM Active Cycles              cycle   280,065.12
    Compute (SM) Throughput           %        29.04
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.92
    Achieved Active Warps Per SM           warp        37.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,338
    Total DRAM Elapsed Cycles        cycle   14,344,192
    Average L1 Active Cycles         cycle   280,065.12
    Total L1 Elapsed Cycles          cycle   11,283,148
    Average L2 Active Cycles         cycle   260,816.94
    Total L2 Elapsed Cycles          cycle    8,491,232
    Average SM Active Cycles         cycle   280,065.12
    Total SM Elapsed Cycles          cycle   11,283,148
    Average SMSP Active Cycles       cycle   279,834.38
    Total SMSP Elapsed Cycles        cycle   45,132,592
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 18, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle      284,655
    Memory Throughput                 %        90.72
    DRAM Throughput                   %        90.72
    Duration                         us       256.83
    L1/TEX Cache Throughput           %        34.31
    L2 Cache Throughput               %        39.17
    SM Active Cycles              cycle   278,902.72
    Compute (SM) Throughput           %        28.91
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.83
    Achieved Active Warps Per SM           warp        37.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,166
    Total DRAM Elapsed Cycles        cycle   14,366,720
    Average L1 Active Cycles         cycle   278,902.72
    Total L1 Elapsed Cycles          cycle   11,333,922
    Average L2 Active Cycles         cycle   260,818.59
    Total L2 Elapsed Cycles          cycle    8,500,096
    Average SM Active Cycles         cycle   278,902.72
    Total SM Elapsed Cycles          cycle   11,333,922
    Average SMSP Active Cycles       cycle   278,565.94
    Total SMSP Elapsed Cycles        cycle   45,335,688
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 19, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      284,815
    Memory Throughput                 %        90.80
    DRAM Throughput                   %        90.80
    Duration                         us       256.74
    L1/TEX Cache Throughput           %        34.42
    L2 Cache Throughput               %        39.19
    SM Active Cycles              cycle   279,884.67
    Compute (SM) Throughput           %        29.00
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.83
    Achieved Active Warps Per SM           warp        37.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,910
    Total DRAM Elapsed Cycles        cycle   14,360,576
    Average L1 Active Cycles         cycle   279,884.67
    Total L1 Elapsed Cycles          cycle   11,299,360
    Average L2 Active Cycles         cycle   260,761.62
    Total L2 Elapsed Cycles          cycle    8,500,288
    Average SM Active Cycles         cycle   279,884.67
    Total SM Elapsed Cycles          cycle   11,299,360
    Average SMSP Active Cycles       cycle   279,869.12
    Total SMSP Elapsed Cycles        cycle   45,197,440
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 20, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      284,988
    Memory Throughput                 %        90.75
    DRAM Throughput                   %        90.75
    Duration                         us       256.86
    L1/TEX Cache Throughput           %        34.51
    L2 Cache Throughput               %        39.15
    SM Active Cycles              cycle   280,049.10
    Compute (SM) Throughput           %        29.06
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.92
    Achieved Active Warps Per SM           warp        37.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,630,008
    Total DRAM Elapsed Cycles        cycle   14,369,792
    Average L1 Active Cycles         cycle   280,049.10
    Total L1 Elapsed Cycles          cycle   11,274,960
    Average L2 Active Cycles         cycle   260,606.75
    Total L2 Elapsed Cycles          cycle    8,505,184
    Average SM Active Cycles         cycle   280,049.10
    Total SM Elapsed Cycles          cycle   11,274,960
    Average SMSP Active Cycles       cycle   279,659.36
    Total SMSP Elapsed Cycles        cycle   45,099,840
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortHistogramKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, unsigned long long, detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5) (400, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,167,216
    Memory Throughput                 %        89.41
    DRAM Throughput                   %        89.41
    Duration                         ms         1.05
    L1/TEX Cache Throughput           %        76.96
    L2 Cache Throughput               %        42.35
    SM Active Cycles              cycle 1,067,124.80
    Compute (SM) Throughput           %        71.20
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    400
    Registers Per Thread             register/thread              47
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            4.10
    # SMs                                         SM              40
    Threads                                   thread          51,200
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        74.59
    Achieved Active Warps Per SM           warp        35.80
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    6,578,144
    Total DRAM Elapsed Cycles        cycle   58,858,496
    Average L1 Active Cycles         cycle 1,067,124.80
    Total L1 Elapsed Cycles          cycle   46,142,364
    Average L2 Active Cycles         cycle    1,064,858
    Total L2 Elapsed Cycles          cycle   34,833,696
    Average SM Active Cycles         cycle 1,067,124.80
    Total SM Elapsed Cycles          cycle   46,142,364
    Average SMSP Active Cycles       cycle 1,066,722.41
    Total SMSP Elapsed Cycles        cycle  184,569,456
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.012%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 7.58% above the average, while the minimum instance value is 5.37% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.987%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 7.56% above the average, while the minimum instance value is 5.26% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.012%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 7.58% above the average, while the minimum instance value is 5.37% below the        
          average.                                                                                                      

  void cub::DeviceRadixSortExclusiveSumKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, unsigned long long>(T2 *) (4, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.86
    SM Frequency                    Ghz         1.08
    Elapsed Cycles                cycle        3,648
    Memory Throughput                 %         2.18
    DRAM Throughput                   %         0.95
    Duration                         us         3.36
    L1/TEX Cache Throughput           %         9.01
    L2 Cache Throughput               %         2.18
    SM Active Cycles              cycle       184.18
    Compute (SM) Throughput           %         0.40
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      4
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.34
    # SMs                                         SM              40
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 90%                                                                                             
          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.48
    Achieved Active Warps Per SM           warp         7.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          220
    Total DRAM Elapsed Cycles        cycle      184,320
    Average L1 Active Cycles         cycle       184.18
    Total L1 Elapsed Cycles          cycle      145,954
    Average L2 Active Cycles         cycle       322.81
    Total L2 Elapsed Cycles          cycle      109,024
    Average SM Active Cycles         cycle       184.18
    Total SM Elapsed Cycles          cycle      145,954
    Average SMSP Active Cycles       cycle       183.68
    Total SMSP Elapsed Cycles        cycle      583,816
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.256%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 55.47% above the average, while the minimum instance value is 88.54% below the      
          average.                                                                                                      

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,608,018
    Memory Throughput                 %        91.84
    DRAM Throughput                   %        91.84
    Duration                         ms         4.15
    L1/TEX Cache Throughput           %        67.67
    L2 Cache Throughput               %        44.81
    SM Active Cycles              cycle 4,600,532.47
    Compute (SM) Throughput           %        67.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.35
    Achieved Active Warps Per SM           warp        23.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,665,998
    Total DRAM Elapsed Cycles        cycle  232,281,088
    Average L1 Active Cycles         cycle 4,600,532.47
    Total L1 Elapsed Cycles          cycle  184,383,090
    Average L2 Active Cycles         cycle 4,288,202.88
    Total L2 Elapsed Cycles          cycle  137,495,264
    Average SM Active Cycles         cycle 4,600,532.47
    Total SM Elapsed Cycles          cycle  184,383,090
    Average SMSP Active Cycles       cycle 4,605,503.89
    Total SMSP Elapsed Cycles        cycle  737,532,360
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,626,736
    Memory Throughput                 %        91.47
    DRAM Throughput                   %        91.47
    Duration                         ms         4.17
    L1/TEX Cache Throughput           %        67.30
    L2 Cache Throughput               %        45.44
    SM Active Cycles              cycle 4,624,565.50
    Compute (SM) Throughput           %        67.41
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.33
    Achieved Active Warps Per SM           warp        23.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,666,930
    Total DRAM Elapsed Cycles        cycle  233,218,048
    Average L1 Active Cycles         cycle 4,624,565.50
    Total L1 Elapsed Cycles          cycle  184,758,790
    Average L2 Active Cycles         cycle    4,300,887
    Total L2 Elapsed Cycles          cycle  138,051,712
    Average SM Active Cycles         cycle 4,624,565.50
    Total SM Elapsed Cycles          cycle  184,758,790
    Average SMSP Active Cycles       cycle 4,619,791.21
    Total SMSP Elapsed Cycles        cycle  739,035,160
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,538,423
    Memory Throughput                 %        93.35
    DRAM Throughput                   %        93.35
    Duration                         ms         4.09
    L1/TEX Cache Throughput           %        68.82
    L2 Cache Throughput               %        43.87
    SM Active Cycles              cycle 4,526,649.33
    Compute (SM) Throughput           %        68.71
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.36
    Achieved Active Warps Per SM           warp        23.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,693,828
    Total DRAM Elapsed Cycles        cycle  228,769,792
    Average L1 Active Cycles         cycle 4,526,649.33
    Total L1 Elapsed Cycles          cycle  181,389,194
    Average L2 Active Cycles         cycle 4,214,539.12
    Total L2 Elapsed Cycles          cycle  135,418,464
    Average SM Active Cycles         cycle 4,526,649.33
    Total SM Elapsed Cycles          cycle  181,389,194
    Average SMSP Active Cycles       cycle 4,528,279.18
    Total SMSP Elapsed Cycles        cycle  725,556,776
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,534,072
    Memory Throughput                 %        93.43
    DRAM Throughput                   %        93.43
    Duration                         ms         4.08
    L1/TEX Cache Throughput           %        68.85
    L2 Cache Throughput               %        43.86
    SM Active Cycles              cycle 4,524,575.42
    Compute (SM) Throughput           %        68.67
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.36
    Achieved Active Warps Per SM           warp        23.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,693,006
    Total DRAM Elapsed Cycles        cycle  228,548,608
    Average L1 Active Cycles         cycle 4,524,575.42
    Total L1 Elapsed Cycles          cycle  181,486,094
    Average L2 Active Cycles         cycle 4,216,498.88
    Total L2 Elapsed Cycles          cycle  135,288,224
    Average SM Active Cycles         cycle 4,524,575.42
    Total SM Elapsed Cycles          cycle  181,486,094
    Average SMSP Active Cycles       cycle 4,525,270.51
    Total SMSP Elapsed Cycles        cycle  725,944,376
    -------------------------- ----------- ------------

  update_ranks_kernel(const int *, const int *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle   67,593,682
    Memory Throughput                 %        43.50
    DRAM Throughput                   %        28.00
    Duration                         ms        60.90
    L1/TEX Cache Throughput           %        25.30
    L2 Cache Throughput               %        43.50
    SM Active Cycles              cycle   67,565,887
    Compute (SM) Throughput           %         1.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.64
    Achieved Active Warps Per SM           warp        38.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle    119,235,090
    Total DRAM Elapsed Cycles        cycle  3,407,210,496
    Average L1 Active Cycles         cycle     67,565,887
    Total L1 Elapsed Cycles          cycle  2,703,239,592
    Average L2 Active Cycles         cycle  62,986,957.09
    Total L2 Elapsed Cycles          cycle  2,016,850,208
    Average SM Active Cycles         cycle     67,565,887
    Total SM Elapsed Cycles          cycle  2,703,239,592
    Average SMSP Active Cycles       cycle  67,561,737.19
    Total SMSP Elapsed Cycles        cycle 10,812,958,368
    -------------------------- ----------- --------------

  void cub::DeviceScanInitKernel<cub::ScanTileState<int, 1>>(T1, int) (427, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.80
    SM Frequency                    Ghz         1.07
    Elapsed Cycles                cycle        3,340
    Memory Throughput                 %        15.25
    DRAM Throughput                   %         4.32
    Duration                         us         3.10
    L1/TEX Cache Throughput           %        26.49
    L2 Cache Throughput               %        15.25
    SM Active Cycles              cycle     1,297.20
    Compute (SM) Throughput           %         6.50
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.9 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    427
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread          54,656
    Uses Green Context                                             0
    Waves Per SM                                                0.89
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        55.87
    Achieved Active Warps Per SM           warp        26.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.13%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (55.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          912
    Total DRAM Elapsed Cycles        cycle      168,960
    Average L1 Active Cycles         cycle     1,297.20
    Total L1 Elapsed Cycles          cycle      131,662
    Average L2 Active Cycles         cycle       868.38
    Total L2 Elapsed Cycles          cycle       99,712
    Average SM Active Cycles         cycle     1,297.20
    Total SM Elapsed Cycles          cycle      131,662
    Average SMSP Active Cycles       cycle     1,224.70
    Total SMSP Elapsed Cycles        cycle      526,648
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.659%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 23.89% above the average, while the minimum instance value is 12.94% below the      
          average.                                                                                                      

  void cub::DeviceScanKernel<scan::policy_hub<int, int, int, unsigned int, thrust::plus<void>>::Policy1000, thrust::device_ptr<int>, thrust::device_ptr<int>, cub::ScanTileState<int, 1>, thrust::plus<void>, cub::NullType, unsigned int, int, 0>(T2, T3, T4, int, T5, T6, T7) (54614, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    2,642,129
    Memory Throughput                 %        78.75
    DRAM Throughput                   %        78.75
    Duration                         ms         2.38
    L1/TEX Cache Throughput           %        44.71
    L2 Cache Throughput               %        35.13
    SM Active Cycles              cycle 2,639,916.88
    Compute (SM) Throughput           %        44.88
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 54,614
    Registers Per Thread             register/thread              84
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            7.70
    # SMs                                         SM              40
    Threads                                   thread       6,990,592
    Uses Green Context                                             0
    Waves Per SM                                              273.07
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           20
    Theoretical Occupancy                     %        41.67
    Achieved Occupancy                        %        40.63
    Achieved Active Warps Per SM           warp        19.50
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 58.33%                                                                                    
          The 5.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (41.7%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   13,114,228
    Total DRAM Elapsed Cycles        cycle  133,224,448
    Average L1 Active Cycles         cycle 2,639,916.88
    Total L1 Elapsed Cycles          cycle  105,203,214
    Average L2 Active Cycles         cycle 2,317,514.47
    Total L2 Elapsed Cycles          cycle   78,855,968
    Average SM Active Cycles         cycle 2,639,916.88
    Total SM Elapsed Cycles          cycle  105,203,214
    Average SMSP Active Cycles       cycle 2,635,034.36
    Total SMSP Elapsed Cycles        cycle  420,812,856
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      285,080
    Memory Throughput                 %        90.73
    DRAM Throughput                   %        90.73
    Duration                         us       256.93
    L1/TEX Cache Throughput           %        34.31
    L2 Cache Throughput               %        39.71
    SM Active Cycles              cycle   280,247.65
    Compute (SM) Throughput           %        28.91
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.88
    Achieved Active Warps Per SM           warp        37.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.12%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,630,104
    Total DRAM Elapsed Cycles        cycle   14,373,888
    Average L1 Active Cycles         cycle   280,247.65
    Total L1 Elapsed Cycles          cycle   11,336,158
    Average L2 Active Cycles         cycle   261,060.28
    Total L2 Elapsed Cycles          cycle    8,508,064
    Average SM Active Cycles         cycle   280,247.65
    Total SM Elapsed Cycles          cycle   11,336,158
    Average SMSP Active Cycles       cycle   280,209.04
    Total SMSP Elapsed Cycles        cycle   45,344,632
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      285,017
    Memory Throughput                 %        90.70
    DRAM Throughput                   %        90.70
    Duration                         us       256.90
    L1/TEX Cache Throughput           %        34.41
    L2 Cache Throughput               %        39.73
    SM Active Cycles              cycle   280,084.17
    Compute (SM) Throughput           %        28.99
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.74
    Achieved Active Warps Per SM           warp        37.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.26%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,372
    Total DRAM Elapsed Cycles        cycle   14,370,816
    Average L1 Active Cycles         cycle   280,084.17
    Total L1 Elapsed Cycles          cycle   11,301,862
    Average L2 Active Cycles         cycle   260,959.47
    Total L2 Elapsed Cycles          cycle    8,506,080
    Average SM Active Cycles         cycle   280,084.17
    Total SM Elapsed Cycles          cycle   11,301,862
    Average SMSP Active Cycles       cycle   280,064.86
    Total SMSP Elapsed Cycles        cycle   45,207,448
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle      284,941
    Memory Throughput                 %        90.64
    DRAM Throughput                   %        90.64
    Duration                         us       257.02
    L1/TEX Cache Throughput           %        34.39
    L2 Cache Throughput               %        39.71
    SM Active Cycles              cycle      280,346
    Compute (SM) Throughput           %        28.99
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.74
    Achieved Active Warps Per SM           warp        37.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.26%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,162
    Total DRAM Elapsed Cycles        cycle   14,379,008
    Average L1 Active Cycles         cycle      280,346
    Total L1 Elapsed Cycles          cycle   11,304,042
    Average L2 Active Cycles         cycle   261,049.62
    Total L2 Elapsed Cycles          cycle    8,508,160
    Average SM Active Cycles         cycle      280,346
    Total SM Elapsed Cycles          cycle   11,304,042
    Average SMSP Active Cycles       cycle   280,240.66
    Total SMSP Elapsed Cycles        cycle   45,216,168
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      285,256
    Memory Throughput                 %        90.69
    DRAM Throughput                   %        90.69
    Duration                         us       257.15
    L1/TEX Cache Throughput           %        34.39
    L2 Cache Throughput               %        39.67
    SM Active Cycles              cycle   280,654.65
    Compute (SM) Throughput           %        28.98
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.77
    Achieved Active Warps Per SM           warp        37.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.23%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,630,424
    Total DRAM Elapsed Cycles        cycle   14,383,104
    Average L1 Active Cycles         cycle   280,654.65
    Total L1 Elapsed Cycles          cycle   11,307,734
    Average L2 Active Cycles         cycle   261,168.88
    Total L2 Elapsed Cycles          cycle    8,513,504
    Average SM Active Cycles         cycle   280,654.65
    Total SM Elapsed Cycles          cycle   11,307,734
    Average SMSP Active Cycles       cycle   280,507.19
    Total SMSP Elapsed Cycles        cycle   45,230,936
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 17, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle      284,848
    Memory Throughput                 %        90.74
    DRAM Throughput                   %        90.74
    Duration                         us       256.77
    L1/TEX Cache Throughput           %        34.45
    L2 Cache Throughput               %        39.74
    SM Active Cycles              cycle   280,114.33
    Compute (SM) Throughput           %        29.04
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.89
    Achieved Active Warps Per SM           warp        37.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,248
    Total DRAM Elapsed Cycles        cycle   14,363,648
    Average L1 Active Cycles         cycle   280,114.33
    Total L1 Elapsed Cycles          cycle   11,284,530
    Average L2 Active Cycles         cycle   260,832.53
    Total L2 Elapsed Cycles          cycle    8,501,376
    Average SM Active Cycles         cycle   280,114.33
    Total SM Elapsed Cycles          cycle   11,284,530
    Average SMSP Active Cycles       cycle   279,728.61
    Total SMSP Elapsed Cycles        cycle   45,138,120
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 18, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle      284,729
    Memory Throughput                 %        90.85
    DRAM Throughput                   %        90.85
    Duration                         us       256.83
    L1/TEX Cache Throughput           %        34.44
    L2 Cache Throughput               %        39.75
    SM Active Cycles              cycle   280,019.08
    Compute (SM) Throughput           %        29.03
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.77
    Achieved Active Warps Per SM           warp        37.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.23%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,631,530
    Total DRAM Elapsed Cycles        cycle   14,366,720
    Average L1 Active Cycles         cycle   280,019.08
    Total L1 Elapsed Cycles          cycle   11,287,520
    Average L2 Active Cycles         cycle   260,565.12
    Total L2 Elapsed Cycles          cycle    8,500,800
    Average SM Active Cycles         cycle   280,019.08
    Total SM Elapsed Cycles          cycle   11,287,520
    Average SMSP Active Cycles       cycle   280,019.63
    Total SMSP Elapsed Cycles        cycle   45,150,080
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 19, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle      284,662
    Memory Throughput                 %        90.77
    DRAM Throughput                   %        90.77
    Duration                         us       256.80
    L1/TEX Cache Throughput           %        34.45
    L2 Cache Throughput               %        39.74
    SM Active Cycles              cycle   280,044.60
    Compute (SM) Throughput           %        29.03
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.93
    Achieved Active Warps Per SM           warp        37.41
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,630,140
    Total DRAM Elapsed Cycles        cycle   14,366,720
    Average L1 Active Cycles         cycle   280,044.60
    Total L1 Elapsed Cycles          cycle   11,287,498
    Average L2 Active Cycles         cycle   260,659.66
    Total L2 Elapsed Cycles          cycle    8,500,704
    Average SM Active Cycles         cycle   280,044.60
    Total SM Elapsed Cycles          cycle   11,287,498
    Average SMSP Active Cycles       cycle   279,949.92
    Total SMSP Elapsed Cycles        cycle   45,149,992
    -------------------------- ----------- ------------

  compute_rank_pairs_kernel(const int *, int *, int, int) (51200, 1, 1)x(256, 1, 1), Context 1, Stream 20, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle      284,427
    Memory Throughput                 %        90.77
    DRAM Throughput                   %        90.77
    Duration                         us       256.70
    L1/TEX Cache Throughput           %        34.70
    L2 Cache Throughput               %        39.76
    SM Active Cycles              cycle   278,214.60
    Compute (SM) Throughput           %        29.23
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread      13,107,200
    Uses Green Context                                             0
    Waves Per SM                                              213.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.01
    Achieved Active Warps Per SM           warp        37.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    1,629,440
    Total DRAM Elapsed Cycles        cycle   14,360,576
    Average L1 Active Cycles         cycle   278,214.60
    Total L1 Elapsed Cycles          cycle   11,208,648
    Average L2 Active Cycles         cycle   260,680.84
    Total L2 Elapsed Cycles          cycle    8,496,992
    Average SM Active Cycles         cycle   278,214.60
    Total SM Elapsed Cycles          cycle   11,208,648
    Average SMSP Active Cycles       cycle   278,301.53
    Total SMSP Elapsed Cycles        cycle   44,834,592
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortHistogramKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, unsigned long long, detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5) (400, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,158,220
    Memory Throughput                 %        90.11
    DRAM Throughput                   %        90.11
    Duration                         ms         1.04
    L1/TEX Cache Throughput           %        76.97
    L2 Cache Throughput               %        43.31
    SM Active Cycles              cycle 1,067,060.68
    Compute (SM) Throughput           %        71.13
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    400
    Registers Per Thread             register/thread              47
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            4.10
    # SMs                                         SM              40
    Threads                                   thread          51,200
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        74.28
    Achieved Active Warps Per SM           warp        35.65
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    6,578,202
    Total DRAM Elapsed Cycles        cycle   58,402,816
    Average L1 Active Cycles         cycle 1,067,060.68
    Total L1 Elapsed Cycles          cycle   46,186,408
    Average L2 Active Cycles         cycle 1,065,174.22
    Total L2 Elapsed Cycles          cycle   34,564,032
    Average SM Active Cycles         cycle 1,067,060.68
    Total SM Elapsed Cycles          cycle   46,186,408
    Average SMSP Active Cycles       cycle 1,066,506.89
    Total SMSP Elapsed Cycles        cycle  184,745,632
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.001%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 7.58% above the average, while the minimum instance value is 5.46% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.835%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 7.40% above the average, while the minimum instance value is 5.45% below    
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.001%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 7.58% above the average, while the minimum instance value is 5.46% below the        
          average.                                                                                                      

  void cub::DeviceRadixSortExclusiveSumKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, unsigned long long>(T2 *) (4, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.83
    SM Frequency                    Ghz         1.09
    Elapsed Cycles                cycle        3,807
    Memory Throughput                 %         2.10
    DRAM Throughput                   %         0.93
    Duration                         us         3.49
    L1/TEX Cache Throughput           %         9.03
    L2 Cache Throughput               %         2.10
    SM Active Cycles              cycle       183.85
    Compute (SM) Throughput           %         0.39
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      4
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.34
    # SMs                                         SM              40
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 90%                                                                                             
          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.48
    Achieved Active Warps Per SM           warp         7.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          222
    Total DRAM Elapsed Cycles        cycle      190,464
    Average L1 Active Cycles         cycle       183.85
    Total L1 Elapsed Cycles          cycle      149,074
    Average L2 Active Cycles         cycle       333.44
    Total L2 Elapsed Cycles          cycle      113,536
    Average SM Active Cycles         cycle       183.85
    Total SM Elapsed Cycles          cycle      149,074
    Average SMSP Active Cycles       cycle       183.90
    Total SMSP Elapsed Cycles        cycle      596,296
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,693,226
    Memory Throughput                 %        90.20
    DRAM Throughput                   %        90.20
    Duration                         ms         4.23
    L1/TEX Cache Throughput           %        66.08
    L2 Cache Throughput               %        47.13
    SM Active Cycles              cycle 4,709,292.30
    Compute (SM) Throughput           %        66.33
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.34
    Achieved Active Warps Per SM           warp        23.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,674,020
    Total DRAM Elapsed Cycles        cycle  236,572,672
    Average L1 Active Cycles         cycle 4,709,292.30
    Total L1 Elapsed Cycles          cycle  187,637,028
    Average L2 Active Cycles         cycle 4,371,532.59
    Total L2 Elapsed Cycles          cycle  140,036,896
    Average SM Active Cycles         cycle 4,709,292.30
    Total SM Elapsed Cycles          cycle  187,637,028
    Average SMSP Active Cycles       cycle 4,674,739.66
    Total SMSP Elapsed Cycles        cycle  750,548,112
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,701,194
    Memory Throughput                 %        90.04
    DRAM Throughput                   %        90.04
    Duration                         ms         4.24
    L1/TEX Cache Throughput           %        66.30
    L2 Cache Throughput               %        47.01
    SM Active Cycles              cycle 4,694,434.62
    Compute (SM) Throughput           %        66.00
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.34
    Achieved Active Warps Per SM           warp        23.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,671,420
    Total DRAM Elapsed Cycles        cycle  236,974,080
    Average L1 Active Cycles         cycle 4,694,434.62
    Total L1 Elapsed Cycles          cycle  188,588,176
    Average L2 Active Cycles         cycle 4,384,795.06
    Total L2 Elapsed Cycles          cycle  140,274,784
    Average SM Active Cycles         cycle 4,694,434.62
    Total SM Elapsed Cycles          cycle  188,588,176
    Average SMSP Active Cycles       cycle 4,691,303.05
    Total SMSP Elapsed Cycles        cycle  754,352,704
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle    4,520,385
    Memory Throughput                 %        93.72
    DRAM Throughput                   %        93.72
    Duration                         ms         4.07
    L1/TEX Cache Throughput           %        68.97
    L2 Cache Throughput               %        43.89
    SM Active Cycles              cycle 4,515,895.40
    Compute (SM) Throughput           %        68.90
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.36
    Achieved Active Warps Per SM           warp        23.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,697,898
    Total DRAM Elapsed Cycles        cycle  227,891,200
    Average L1 Active Cycles         cycle 4,515,895.40
    Total L1 Elapsed Cycles          cycle  180,739,758
    Average L2 Active Cycles         cycle 4,201,240.31
    Total L2 Elapsed Cycles          cycle  134,895,488
    Average SM Active Cycles         cycle 4,515,895.40
    Total SM Elapsed Cycles          cycle  180,739,758
    Average SMSP Active Cycles       cycle 4,514,768.82
    Total SMSP Elapsed Cycles        cycle  722,959,032
    -------------------------- ----------- ------------

  void cub::DeviceRadixSortOnesweepKernel<radix::policy_hub<int, int, unsigned long long>::Policy1000, 0, int, int, unsigned long long, int, int, detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8) (16063, 1, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    4,546,583
    Memory Throughput                 %        93.16
    DRAM Throughput                   %        93.16
    Duration                         ms         4.10
    L1/TEX Cache Throughput           %        68.57
    L2 Cache Throughput               %        43.70
    SM Active Cycles              cycle 4,544,866.47
    Compute (SM) Throughput           %        68.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 16,063
    Registers Per Thread             register/thread              79
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           28.16
    # SMs                                         SM              40
    Threads                                   thread       6,168,192
    Uses Green Context                                             0
    Waves Per SM                                              200.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.37
    Achieved Active Warps Per SM           warp        23.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   26,686,612
    Total DRAM Elapsed Cycles        cycle  229,179,392
    Average L1 Active Cycles         cycle 4,544,866.47
    Total L1 Elapsed Cycles          cycle  181,763,210
    Average L2 Active Cycles         cycle 4,224,897.81
    Total L2 Elapsed Cycles          cycle  135,660,960
    Average SM Active Cycles         cycle 4,544,866.47
    Total SM Elapsed Cycles          cycle  181,763,210
    Average SMSP Active Cycles       cycle 4,541,002.38
    Total SMSP Elapsed Cycles        cycle  727,052,840
    -------------------------- ----------- ------------

  update_ranks_kernel(const int *, const int *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle    67,606,271
    Memory Throughput                 %         43.62
    DRAM Throughput                   %         27.99
    Duration                         ms         60.91
    L1/TEX Cache Throughput           %         25.62
    L2 Cache Throughput               %         43.62
    SM Active Cycles              cycle 66,865,298.48
    Compute (SM) Throughput           %          1.96
    ----------------------- ----------- -------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.27
    Achieved Active Warps Per SM           warp        38.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.73%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle    119,243,288
    Total DRAM Elapsed Cycles        cycle  3,407,846,400
    Average L1 Active Cycles         cycle  66,865,298.48
    Total L1 Elapsed Cycles          cycle  2,670,559,394
    Average L2 Active Cycles         cycle  64,026,439.53
    Total L2 Elapsed Cycles          cycle  2,017,226,048
    Average SM Active Cycles         cycle  66,865,298.48
    Total SM Elapsed Cycles          cycle  2,670,559,394
    Average SMSP Active Cycles       cycle  66,752,245.62
    Total SMSP Elapsed Cycles        cycle 10,682,237,576
    -------------------------- ----------- --------------

  void cub::DeviceScanInitKernel<cub::ScanTileState<int, 1>>(T1, int) (427, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.88
    SM Frequency                    Ghz         1.09
    Elapsed Cycles                cycle        3,243
    Memory Throughput                 %        15.56
    DRAM Throughput                   %         3.39
    Duration                         us         2.98
    L1/TEX Cache Throughput           %        26.81
    L2 Cache Throughput               %        15.56
    SM Active Cycles              cycle     1,281.53
    Compute (SM) Throughput           %         6.69
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.9 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    427
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread          54,656
    Uses Green Context                                             0
    Waves Per SM                                                0.89
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        56.04
    Achieved Active Warps Per SM           warp        26.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (56.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          694
    Total DRAM Elapsed Cycles        cycle      163,840
    Average L1 Active Cycles         cycle     1,281.53
    Total L1 Elapsed Cycles          cycle      128,244
    Average L2 Active Cycles         cycle       886.47
    Total L2 Elapsed Cycles          cycle       99,680
    Average SM Active Cycles         cycle     1,281.53
    Total SM Elapsed Cycles          cycle      128,244
    Average SMSP Active Cycles       cycle     1,211.71
    Total SMSP Elapsed Cycles        cycle      512,976
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.673%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 23.45% above the average, while the minimum instance value is 12.35% below the      
          average.                                                                                                      

  void cub::DeviceScanKernel<scan::policy_hub<int, int, int, unsigned int, thrust::plus<void>>::Policy1000, thrust::device_ptr<int>, thrust::device_ptr<int>, cub::ScanTileState<int, 1>, thrust::plus<void>, cub::NullType, unsigned int, int, 0>(T2, T3, T4, int, T5, T6, T7) (54614, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    2,631,249
    Memory Throughput                 %        79.08
    DRAM Throughput                   %        79.08
    Duration                         ms         2.37
    L1/TEX Cache Throughput           %        44.87
    L2 Cache Throughput               %        34.29
    SM Active Cycles              cycle 2,629,651.50
    Compute (SM) Throughput           %        45.25
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 54,614
    Registers Per Thread             register/thread              84
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            7.70
    # SMs                                         SM              40
    Threads                                   thread       6,990,592
    Uses Green Context                                             0
    Waves Per SM                                              273.07
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           20
    Theoretical Occupancy                     %        41.67
    Achieved Occupancy                        %        40.65
    Achieved Active Warps Per SM           warp        19.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 58.33%                                                                                    
          The 5.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (41.7%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   13,112,966
    Total DRAM Elapsed Cycles        cycle  132,655,104
    Average L1 Active Cycles         cycle 2,629,651.50
    Total L1 Elapsed Cycles          cycle  104,337,460
    Average L2 Active Cycles         cycle 2,370,042.59
    Total L2 Elapsed Cycles          cycle   80,795,712
    Average SM Active Cycles         cycle 2,629,651.50
    Total SM Elapsed Cycles          cycle  104,337,460
    Average SMSP Active Cycles       cycle 2,626,528.37
    Total SMSP Elapsed Cycles        cycle  417,349,840
    -------------------------- ----------- ------------

  compute_rank_kernel(const int *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle    34,169,905
    Memory Throughput                 %         42.71
    DRAM Throughput                   %         27.38
    Duration                         ms         30.78
    L1/TEX Cache Throughput           %         31.02
    L2 Cache Throughput               %         42.71
    SM Active Cycles              cycle 34,164,585.88
    Compute (SM) Throughput           %          2.55
    ----------------------- ----------- -------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.65
    Achieved Active Warps Per SM           warp        38.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle    58,941,064
    Total DRAM Elapsed Cycles        cycle 1,722,411,008
    Average L1 Active Cycles         cycle 34,164,585.88
    Total L1 Elapsed Cycles          cycle 1,367,818,614
    Average L2 Active Cycles         cycle 32,570,493.84
    Total L2 Elapsed Cycles          cycle 1,049,108,160
    Average SM Active Cycles         cycle 34,164,585.88
    Total SM Elapsed Cycles          cycle 1,367,818,614
    Average SMSP Active Cycles       cycle 34,158,482.36
    Total SMSP Elapsed Cycles        cycle 5,471,274,456
    -------------------------- ----------- -------------

  compute_lcp_kernel(const unsigned char *, const int *, const int *, int *, int) (409600, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.11
    Elapsed Cycles                cycle    58,236,165
    Memory Throughput                 %         38.99
    DRAM Throughput                   %         32.87
    Duration                         ms         52.47
    L1/TEX Cache Throughput           %         19.89
    L2 Cache Throughput               %         38.99
    SM Active Cycles              cycle 58,201,893.30
    Compute (SM) Throughput           %          2.68
    ----------------------- ----------- -------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread     104,857,600
    Uses Green Context                                             0
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        84.83
    Achieved Active Warps Per SM           warp        40.72
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 15.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (84.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle   120,601,022
    Total DRAM Elapsed Cycles        cycle 2,935,523,328
    Average L1 Active Cycles         cycle 58,201,893.30
    Total L1 Elapsed Cycles          cycle 2,328,031,016
    Average L2 Active Cycles         cycle 55,855,824.34
    Total L2 Elapsed Cycles          cycle 1,788,007,648
    Average SM Active Cycles         cycle 58,201,893.30
    Total SM Elapsed Cycles          cycle 2,328,031,016
    Average SMSP Active Cycles       cycle 58,225,834.65
    Total SMSP Elapsed Cycles        cycle 9,312,124,064
    -------------------------- ----------- -------------

  void core::_kernel_agent<__reduce::DrainAgent<int>, cub::GridQueue<unsigned int>, int>(T2...) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.83
    SM Frequency                    Ghz         1.08
    Elapsed Cycles                cycle        2,603
    Memory Throughput                 %         1.17
    DRAM Throughput                   %         0.28
    Duration                         us         2.40
    L1/TEX Cache Throughput           %        56.67
    L2 Cache Throughput               %         1.17
    SM Active Cycles              cycle        21.18
    Compute (SM) Throughput           %         0.00
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread               1
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 96.88%                                                                                          
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block          128
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp            1
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.75%                                                                                    
          The difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (33.3%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           46
    Total DRAM Elapsed Cycles        cycle      131,072
    Average L1 Active Cycles         cycle        21.18
    Total L1 Elapsed Cycles          cycle      118,504
    Average L2 Active Cycles         cycle       123.66
    Total L2 Elapsed Cycles          cycle       79,872
    Average SM Active Cycles         cycle        21.18
    Total SM Elapsed Cycles          cycle      118,504
    Average SMSP Active Cycles       cycle         5.19
    Total SMSP Elapsed Cycles        cycle      474,016
    -------------------------- ----------- ------------

  void core::_kernel_agent<__reduce::ReduceAgent<thrust::zip_iterator<cuda::tuple<thrust::device_ptr<int>, thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>>>, cuda::tuple<int, long> *, cuda::tuple<int, long>, int, __extrema::arg_max_f<int, long, thrust::less<int>>>, thrust::zip_iterator<cuda::tuple<thrust::device_ptr<int>, thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>>>, cuda::tuple<int, long> *, int, cub::GridEvenShare<int>, cub::GridQueue<unsigned int>, __extrema::arg_max_f<int, long, thrust::less<int>>>(T2...) (240, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle    1,079,534
    Memory Throughput                 %        96.73
    DRAM Throughput                   %        96.73
    Duration                         us       972.58
    L1/TEX Cache Throughput           %        30.73
    L2 Cache Throughput               %        39.83
    SM Active Cycles              cycle 1,073,021.20
    Compute (SM) Throughput           %        33.96
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    240
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread          61,440
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.86
    Achieved Active Warps Per SM           warp        47.93
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    6,578,946
    Total DRAM Elapsed Cycles        cycle   54,413,312
    Average L1 Active Cycles         cycle 1,073,021.20
    Total L1 Elapsed Cycles          cycle   43,062,288
    Average L2 Active Cycles         cycle 1,025,689.75
    Total L2 Elapsed Cycles          cycle   33,144,960
    Average SM Active Cycles         cycle 1,073,021.20
    Total SM Elapsed Cycles          cycle   43,062,288
    Average SMSP Active Cycles       cycle 1,072,582.13
    Total SMSP Elapsed Cycles        cycle  172,249,152
    -------------------------- ----------- ------------

  void core::_kernel_agent<__reduce::ReduceAgent<cuda::tuple<int, long> *, cuda::tuple<int, long> *, cuda::tuple<int, long>, int, __extrema::arg_max_f<int, long, thrust::less<int>>>, cuda::tuple<int, long> *, cuda::tuple<int, long> *, int, __extrema::arg_max_f<int, long, thrust::less<int>>>(T2...) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.93
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle        5,526
    Memory Throughput                 %         3.43
    DRAM Throughput                   %         1.49
    Duration                         us         5.02
    L1/TEX Cache Throughput           %        12.78
    L2 Cache Throughput               %         3.43
    SM Active Cycles              cycle        94.12
    Compute (SM) Throughput           %         0.18
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Threads                                   thread             256
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.29
    Achieved Active Warps Per SM           warp         6.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.71%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          518
    Total DRAM Elapsed Cycles        cycle      278,528
    Average L1 Active Cycles         cycle        94.12
    Total L1 Elapsed Cycles          cycle      218,608
    Average L2 Active Cycles         cycle       571.38
    Total L2 Elapsed Cycles          cycle      169,536
    Average SM Active Cycles         cycle        94.12
    Total SM Elapsed Cycles          cycle      218,608
    Average SMSP Active Cycles       cycle        79.64
    Total SMSP Elapsed Cycles        cycle      874,432
    -------------------------- ----------- ------------

